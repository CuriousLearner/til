[{"path": "python_handle-cors-simple-http-server.md", "topic": "python", "title": "Handle CORS in a Simple HTTP Server", "url": "https://github.com/CuriousLearner/til/blob/main/python/handle-cors-simple-http-server.md", "body": "This is a simple HTTP server that serves the current directory and handles CORS by adding the `Access-Control-Allow-Origin` header to the response. This is useful when you are working with frontend applications that need to make requests to a server running on a different domain.\n\n```python\nfrom http.server import SimpleHTTPRequestHandler\nimport socketserver\n\nclass CORSRequestHandler(SimpleHTTPRequestHandler):\n    def end_headers(self):\n        self.send_header('Access-Control-Allow-Origin', '*')\n        super().end_headers()\n\nwith socketserver.TCPServer((\"\", 8000), CORSRequestHandler) as httpd:\n    print(\"Serving at port 8000\")\n    httpd.serve_forever()\n```\n\nSave this script as `server.py` and run it using `python server.py`. This will start a simple HTTP server that serves the current directory and handles CORS by allowing requests from any origin.\n\nYou can access the server at `http://localhost:8000` and make requests from your frontend application without any CORS restrictions.\n\nThis is a quick way to set up a simple HTTP server for development purposes and handle CORS without having to configure a full-fledged web server like Apache or Nginx.\n\n**Note:** This server is not suitable for production use as it lacks security features and is not optimized for performance. It is intended for development purposes only.", "created": "2025-02-28T07:10:41+05:30", "created_utc": "2025-02-28T01:40:41+00:00", "updated": "2025-02-28T07:10:41+05:30", "updated_utc": "2025-02-28T01:40:41+00:00"}, {"path": "python_update-dependencies-in-requirements.md", "topic": "python", "title": "Update dependencies in requirements.txt", "url": "https://github.com/CuriousLearner/til/blob/main/python/update-dependencies-in-requirements.md", "body": "You can use `pur` to update all dependencies in a requirements file like:\n\n```bash\npur -r requirements.txt\n```\n\n`pur` never modifies your environment or installed packages, it just updates the txt file with latest dependencies. So you can install requirements with `pip install -r requirements.txt`, afterwards.", "created": "2025-02-28T07:10:41+05:30", "created_utc": "2025-02-28T01:40:41+00:00", "updated": "2025-02-28T07:10:41+05:30", "updated_utc": "2025-02-28T01:40:41+00:00"}, {"path": "python_pathlib-path-expand-user.md", "topic": "python", "title": "Expand home directory paths in `pathlib.Path`", "url": "https://github.com/CuriousLearner/til/blob/main/python/pathlib-path-expand-user.md", "body": "To expand home directory paths in `pathlib.Path`, use the `expanduser()` method. This converts the `~` to the full home directory path.\n\n```python\nfrom pathlib import Path\n\npath = Path(\"~/.private_keys/project-x.2024-06-23.private-key.pem\").expanduser()\n\n# Users/CuriousLearner/.private_keys/project-x.2024-06-23.private-key.pem\n```", "created": "2024-06-23T09:25:31+05:30", "created_utc": "2024-06-23T03:55:31+00:00", "updated": "2024-06-23T09:25:31+05:30", "updated_utc": "2024-06-23T03:55:31+00:00"}, {"path": "python_timedelta-total-seconds-vs-seconds.md", "topic": "python", "title": "`timedelta.total_seconds()` vs `timedelta.seconds` \u2013 A Critical Difference", "url": "https://github.com/CuriousLearner/til/blob/main/python/timedelta-total-seconds-vs-seconds.md", "body": "When working with Python's `timedelta` objects, there's a critical difference between `.seconds` and `.total_seconds()` that can lead to subtle bugs.\n\n## The Problem\n\n- `.seconds` \u2013 Returns **only the seconds component** (0-86399) of the timedelta, excluding days\n- `.total_seconds()` \u2013 Returns the **total duration** in seconds as a float\n\n## Example\n\n```python\nfrom datetime import timedelta\n\n# A duration of 2 days and 5 seconds\ntd = timedelta(days=2, seconds=5)\n\nprint(td.seconds)          # Output: 5 (only the seconds component!)\nprint(td.total_seconds())  # Output: 172805.0 (2 days + 5 seconds = 172800 + 5)\n```\n\n## Common Bug Pattern\n\n```python\n# WRONG - This only checks the seconds component\nif some_timedelta.seconds > 3600:\n    # This will NEVER trigger for durations like \"2 days\"\n    # because .seconds only returns 0-86399\n    do_something()\n\n# CORRECT - This checks the entire duration\nif some_timedelta.total_seconds() > 3600:\n    do_something()\n```\n\n## Why This Matters\n\nA timedelta has three components: `days`, `seconds`, and `microseconds`. The `.seconds` attribute only gives you the middle component (normalized to 0-86399), while `.total_seconds()` converts the entire duration to seconds.\n\n## Rule of Thumb\n\n**Always use `.total_seconds()` when you need the complete duration.** Only use `.seconds` when you specifically need just the seconds component of a time interval.", "created": "2025-11-26T05:46:18+05:30", "created_utc": "2025-11-26T00:16:18+00:00", "updated": "2025-12-08T09:54:37+05:30", "updated_utc": "2025-12-08T04:24:37+00:00"}, {"path": "wagtail_get-blocks-in-streamblock.md", "topic": "wagtail", "title": "Get blocks in StreamBlock using `blocks_by_name` & `first_block_by_name`", "url": "https://github.com/CuriousLearner/til/blob/main/wagtail/get-blocks-in-streamblock.md", "body": "If you're using a StreamField, you can access the blocks using the `blocks_by_name` method. This will return a list of the blocks with the given name.\n\nExample:\n\n```python\n@register_snippet\nclass Menu(models.Model):\n\n    content = StreamField(\n        [\n            (\"column\", ContentColumnBlock()),\n            (\"sidebar\", SidebarBlock()),\n        ]\n    )\n    def clean(self):\n        super().clean()\n        if len(self.content.blocks_by_name(\"sidebar\")) > 1:\n            raise ValidationError(\"Only one sidebar block is allowed.\")\n        for column_block in self.content.blocks_by_name(\"column\"):\n            if len(column_block.value[\"column\"]) > 3:\n                raise ValidationError(\n                    \"Only up to three choices in each column is allowed.\"\n                )\n```\n\nYou can also use the `first_block_by_name` method if you only want to get the first block with the given name.\n\nExample:\n\n```python\n{{ menu.content.blocks.first_block_by_name(\"column\") }}\n```\n\nThese methods can also be used in templates to access the blocks.", "created": "2024-04-25T00:48:36+05:30", "created_utc": "2024-04-24T19:18:36+00:00", "updated": "2024-04-25T00:51:11+05:30", "updated_utc": "2024-04-24T19:21:11+00:00"}, {"path": "wagtail_lazily-refer-snippet.md", "topic": "wagtail", "title": "Lazily Refer Snippet in blocks", "url": "https://github.com/CuriousLearner/til/blob/main/wagtail/lazily-refer-snippet.md", "body": "`SnippetChooserBlock` in Django Wagtail can cleverly sidestep circular dependencies. By lazily referring to a snippet, you can effectively dodge those pesky circular dependencies that can otherwise tangle up your code. Just set up your `SnippetChooserBlock` like this:\n\n```python\nfrom wagtail.snippets.blocks import SnippetChooserBlock\n\nSnippetChooserBlock(target_model=\"app.ContentSnippet\")\n```\n\nAnd watch as your code becomes cleaner and more maintainable! No more headaches trying to unravel circular dependencies.", "created": "2025-02-28T07:10:41+05:30", "created_utc": "2025-02-28T01:40:41+00:00", "updated": "2025-02-28T07:10:41+05:30", "updated_utc": "2025-02-28T01:40:41+00:00"}, {"path": "wagtail_many-to-many-relationship.md", "topic": "wagtail", "title": "Many to Many relationship in Wagtail", "url": "https://github.com/CuriousLearner/til/blob/main/wagtail/many-to-many-relationship.md", "body": "A Many-to-Many relationship can be implemented using a `ParentalManyToManyField` in Wagtail. This field is designed for use within a `PageModel` and allows you to create a many-to-many relationship between different pages. The `ParentalManyToManyField` allows the relations between m2m linked objects to be stored in memory without writing to the database.\n\n```python\nfrom modelcluster.fields import ParentalManyToManyField\n```\n\nYou can then define the `ParentalManyToManyField` in your Wagtail model, specifying the related page model that you want to establish the many-to-many relationship with.\n\n**Caveat**: If you're using snippets, make sure the snippet is using the `ParentalManyToManyField`, otherwise it won't render correctly with `FieldPanel`. Furthermore, the snippet cannot inherit from `models.Model`, otherwise the m2m relationship will not be saved. They should inherit from `modelcluster.models.ClusterableModel` instead.\n\n```python\nfrom modelcluster.fields import ParentalManyToManyField\nfrom modelcluster.models import ClusterableModel\n\nfrom wagtail.models import Page\nfrom wagtail.snippets.models import register_snippet\n\n\n@register_snippet\nclass MySnippet(ClusterableModel):\n    my_pages = ParentalManyToManyField(Page, related_name=\"+\")\n```\n\nThis will create a many-to-many relationship between the `MySnippet` model and the `Page` model.", "created": "2024-04-25T00:46:51+05:30", "created_utc": "2024-04-24T19:16:51+00:00", "updated": "2024-04-25T00:46:51+05:30", "updated_utc": "2024-04-24T19:16:51+00:00"}, {"path": "vlc_cast-video-to-chromecast.md", "topic": "vlc", "title": "Cast video to chromecast", "url": "https://github.com/CuriousLearner/til/blob/main/vlc/cast-video-to-chromecast.md", "body": "> To cast your videos from your Mac to Chromecast, all you need to do is Open VLC and click `Playback` > `Renderer` from the Apple menu bar. Then select the name of your Chromecast device from the list.", "created": "2023-02-13T22:41:29+05:30", "created_utc": "2023-02-13T17:11:29+00:00", "updated": "2023-02-13T22:41:29+05:30", "updated_utc": "2023-02-13T17:11:29+00:00"}, {"path": "npm_clean-install-for-ci-pipelines.md", "topic": "npm", "title": "`npm ci` (clean install) for CI pipelines", "url": "https://github.com/CuriousLearner/til/blob/main/npm/clean-install-for-ci-pipelines.md", "body": "Just learned about `npm ci` command! It stands for 'clean install' and is super useful for Continuous Integration setups. Unlike `npm install`, it skips the dependency resolution step by using the existing `package-lock.json` or `npm-shrinkwrap.json`. This ensures faster, consistent builds with exact dependencies.\n\n```bash\nnpm ci\n```", "created": "2025-02-28T07:10:41+05:30", "created_utc": "2025-02-28T01:40:41+00:00", "updated": "2025-12-08T09:54:37+05:30", "updated_utc": "2025-12-08T04:24:37+00:00"}, {"path": "psql_restore-via-template.md", "topic": "psql", "title": "Restore database via template", "url": "https://github.com/CuriousLearner/til/blob/main/psql/restore-via-template.md", "body": "To restore a PostgreSQL database using a template, you can use the `createdb` command with the `--template` option. This method is faster than traditional backup and restore because it copies the database files directly, avoiding the overhead of SQL-based operations.\n\nFor PostgreSQL 15, the createdb command uses the `WAL_LOG` method by default. This method copies the database block by block and logs each block in the write-ahead log. It's the most efficient strategy for small template databases.\n\nThe older `FILE_COPY` method writes a small record to the write-ahead log for each tablespace, representing a directory copy at the filesystem level.\n\nExample:\n\nTo create a new database `newdb` from the template `templatedb`:\n\n```bash\ncreatedb newdb --template=templatedb\n```\n\nThis command efficiently copies the schema and data from `templatedb` to `newdb`.\n\nWith create database command,\n\n```sql\ncreate database newdb template templatedb;\n```", "created": "2024-06-23T09:43:20+05:30", "created_utc": "2024-06-23T04:13:20+00:00", "updated": "2024-06-23T09:43:20+05:30", "updated_utc": "2024-06-23T04:13:20+00:00"}, {"path": "psql_partial-index.md", "topic": "psql", "title": "Partial Index", "url": "https://github.com/CuriousLearner/til/blob/main/psql/partial-index.md", "body": "A partial index in PostgreSQL is an index built over a subset of rows in a table, defined by a `WHERE` clause. This makes the index smaller, faster to scan, and cheaper to maintain compared to a full index.\n\n## Syntax\n\n```sql\nCREATE INDEX index_name ON table_name (column_name)\nWHERE condition;\n```\n\n## Example\n\nConsider an `orders` table where most queries filter for active (unshipped) orders, but the vast majority of rows are already shipped:\n\n```sql\nCREATE INDEX idx_orders_pending ON orders (created_at)\nWHERE status != 'shipped';\n```\n\nThis index only includes rows where `status != 'shipped'`, so it stays small even as the table grows with millions of shipped orders.\n\n## When to use partial indexes\n\n- **Skewed data distributions** \u2014 when queries target a small subset of rows (e.g., `WHERE active = true` on a table that's 95% inactive)\n- **Soft deletes** \u2014 index only non-deleted rows: `WHERE deleted_at IS NULL`\n- **Status-based filtering** \u2014 index only rows in a specific state: `WHERE status = 'pending'`\n- **Unique constraints on a subset** \u2014 enforce uniqueness only where it matters:\n\n```sql\nCREATE UNIQUE INDEX idx_unique_active_email ON users (email)\nWHERE deleted_at IS NULL;\n```\n\nThis allows multiple rows with the same email as long as only one is non-deleted.\n\n## Verifying the index is used\n\nUse `EXPLAIN ANALYZE` to confirm PostgreSQL uses the partial index:\n\n```sql\nEXPLAIN ANALYZE\nSELECT * FROM orders WHERE status != 'shipped' AND created_at > '2025-01-01';\n```\n\nLook for `Index Scan using idx_orders_pending` in the output. If the query's `WHERE` clause doesn't imply the index's predicate, PostgreSQL won't use it.\n\n## Important notes\n\n- The query's `WHERE` clause must logically match or imply the index's condition for the planner to consider it.\n- Partial indexes reduce index size and write amplification since inserts/updates to rows outside the predicate don't touch the index at all.\n- They can be combined with any index type (`btree`, `gin`, `gist`, etc.).", "created": "2026-02-26T11:52:53+05:30", "created_utc": "2026-02-26T06:22:53+00:00", "updated": "2026-02-26T11:52:53+05:30", "updated_utc": "2026-02-26T06:22:53+00:00"}, {"path": "psql_vacuum-analyze.md", "topic": "psql", "title": "VACUUM ANALYZE in PostgreSQL", "url": "https://github.com/CuriousLearner/til/blob/main/psql/vacuum-analyze.md", "body": "PostgreSQL uses `VACUUM ANALYZE` to maintain database health and update query planner statistics.\n\n## What it does\n\n**VACUUM**: Reclaims storage occupied by dead tuples (deleted/updated rows)\n- Prevents transaction ID wraparound\n- Makes space available for reuse (doesn't return it to OS by default)\n- Required for bloated tables\n\n**ANALYZE**: Updates statistics used by the query planner\n- Collects data about value distributions\n- Helps planner choose optimal query execution plans\n- Critical for good query performance\n\n## Basic usage\n\n```sql\n-- Analyze all tables in current database\nVACUUM ANALYZE;\n\n-- Analyze specific table\nVACUUM ANALYZE table_name;\n\n-- Just analyze without vacuum\nANALYZE table_name;\n\n-- Verbose output\nVACUUM ANALYZE VERBOSE table_name;\n```\n\n## When to use\n\n### Run VACUUM ANALYZE when:\n- After bulk INSERT/UPDATE/DELETE operations\n- Query performance degrades unexpectedly\n- `EXPLAIN ANALYZE` shows large discrepancies between estimated and actual rows\n- After schema changes or index creation\n\n### Run VACUUM FULL when:\n```sql\n-- Reclaim disk space (locks table, rewrites entire table)\nVACUUM FULL table_name;\n```\n**Warning**: `VACUUM FULL` locks the table and can take a long time. Use sparingly.\n\n## Auto-vacuum\n\nPostgreSQL runs auto-vacuum in the background by default, but you may need manual runs:\n\n```sql\n-- Check auto-vacuum settings\nSHOW autovacuum;\n\n-- Check last auto-vacuum/analyze time\nSELECT schemaname, relname, last_vacuum, last_autovacuum,\n       last_analyze, last_autoanalyze\nFROM pg_stat_user_tables\nWHERE relname = 'table_name';\n```\n\n## Identifying tables that need attention\n\n```sql\n-- Tables with most dead tuples\nSELECT schemaname, relname, n_dead_tup, n_live_tup,\n       round(n_dead_tup * 100.0 / NULLIF(n_live_tup + n_dead_tup, 0), 2) AS dead_pct\nFROM pg_stat_user_tables\nWHERE n_dead_tup > 1000\nORDER BY n_dead_tup DESC\nLIMIT 10;\n\n-- Tables not analyzed recently\nSELECT schemaname, relname, last_analyze, last_autoanalyze\nFROM pg_stat_user_tables\nWHERE last_analyze IS NULL OR last_analyze < NOW() - INTERVAL '7 days'\nORDER BY last_analyze NULLS FIRST;\n```\n\n## Django integration\n\n```python\nfrom django.db import connection\n\n# Run VACUUM ANALYZE from Django\nwith connection.cursor() as cursor:\n    cursor.execute(\"VACUUM ANALYZE myapp_mymodel;\")\n```\n\n## Best practices\n\n1. **Let auto-vacuum handle routine maintenance** for most cases\n2. **Manually run after bulk operations** (large imports, migrations)\n3. **Monitor statistics age** - stale stats lead to poor query plans\n4. **Use VACUUM ANALYZE, not VACUUM FULL** unless you need to reclaim disk space\n5. **Schedule during low-traffic periods** for large tables\n6. **Check dead tuple ratios** regularly to spot issues early", "created": "2025-10-03T04:16:21+05:30", "created_utc": "2025-10-02T22:46:21+00:00", "updated": "2025-10-03T04:16:21+05:30", "updated_utc": "2025-10-02T22:46:21+00:00"}, {"path": "psql_dump-and-restore.md", "topic": "psql", "title": "Dump and Restore database", "url": "https://github.com/CuriousLearner/til/blob/main/psql/dump-and-restore.md", "body": "To dump:\n\n```bash\nPGPASSWORD=<password> pg_dump --username <username> <dbname> > dump.sql\n```\n\nTo restore:\n\n```bash\nPGPASSWORD=<password> psql --username <username> <dbname> -p <port> -h <host> < dump.sql\n```", "created": "2023-07-08T01:00:47+05:30", "created_utc": "2023-07-07T19:30:47+00:00", "updated": "2025-02-27T19:36:39-05:00", "updated_utc": "2025-02-28T00:36:39+00:00"}, {"path": "pytest_run-failed-test-and-exit-at-first-failure.md", "topic": "pytest", "title": "Run only failed tests and stop at first failure in pytest", "url": "https://github.com/CuriousLearner/til/blob/main/pytest/run-failed-test-and-exit-at-first-failure.md", "body": "Use `pytest -x --lf` to speed up debugging:\n\n* `--lf (last failed)` runs only the tests that failed in the previous run.\n* `-x (exit first)` stops execution at the first failure.\n\nIf all tests passed in the last run, `--lf` runs all tests as a fallback.\n\nUseful for quickly iterating on failing tests without running the entire test suite.", "created": "2025-03-04T03:59:59+05:30", "created_utc": "2025-03-03T22:29:59+00:00", "updated": "2025-03-04T03:59:59+05:30", "updated_utc": "2025-03-03T22:29:59+00:00"}, {"path": "pytest_fixture-autouse.md", "topic": "pytest", "title": "Autouse fixtures for common test dependencies", "url": "https://github.com/CuriousLearner/til/blob/main/pytest/fixture-autouse.md", "body": "Fixtures can be marked as autouse, meaning they will be invoked for all tests in a module without a direct dependency on them. This can be useful for setting up common test dependencies, such as a temporary directory or a database connection.\n\n```python\nimport pytest\n\n@pytest.fixture(autouse=True)\ndef setup_database():\n    print(\"\\nSetup database\")\n    yield\n    print(\"\\nTeardown database\")\n\ndef test_insert_data():\n    print(\"Inserting data\")\n\ndef test_delete_data():\n    print(\"Deleting data\")\n```\n\nWhen running the tests, the output will be:\n\n```shell\nSetup database\nInserting data\nTeardown database\nSetup database\nDeleting data\nTeardown database\n```\n\nIn the above example, the `setup_database` fixture is marked as autouse, so it will be invoked for all tests in the module. The fixture sets up the database before each test and tears it down after each test.", "created": "2025-02-28T07:10:41+05:30", "created_utc": "2025-02-28T01:40:41+00:00", "updated": "2025-02-28T07:10:41+05:30", "updated_utc": "2025-02-28T01:40:41+00:00"}, {"path": "pytest_pythonpath-import-mode-configuration.md", "topic": "pytest", "title": "Understanding pytest pythonpath and import-mode configuration", "url": "https://github.com/CuriousLearner/til/blob/main/pytest/pythonpath-import-mode-configuration.md", "body": "## What `pythonpath` does\n\nThe `pythonpath` configuration option modifies `sys.path` by adding specified directories to Python's module search path:\n\n```toml\n[tool.pytest.ini_options]\npythonpath = [\".\"]\n```\n\n- Adds directories to `sys.path` relative to the rootdir\n- Useful for src layout projects or avoiding PYTHONPATH environment variables\n- Available since pytest 7.0.0\n- Keeps path configuration in the repo and distributes to other developers\n\n## `pytest` vs `python -m pytest` difference\n\n### `python -m pytest`:\n- Automatically adds the current directory to `sys.path`\n- Standard Python behavior when running modules with `-m`\n- More reliable for local development without installed packages\n\n### `pytest` command:\n- Relies on existing Python environment configuration\n- Uses pytest's own import mechanisms (prepend mode by default)\n- Assumes modules are properly installed or `sys.path` is configured\n\n## What `--import-mode=importlib` does\n\n```toml\n[tool.pytest.ini_options]\naddopts = \"--import-mode=importlib\"\n```\n\n- Circumvents the standard Python way of using modules and `sys.path`\n- Doesn't modify `sys.path` or `sys.modules` during test discovery\n- Allows test modules to have non-unique names\n- Makes behavior less surprising but has some trade-offs\n- Uses Python's importlib instead of modifying the module system\n\nThe `pythonpath` option essentially provides what `python -m pytest` does automatically - ensuring your local modules are discoverable during testing.", "created": "2025-09-18T23:46:19+05:30", "created_utc": "2025-09-18T18:16:19+00:00", "updated": "2025-09-18T23:46:19+05:30", "updated_utc": "2025-09-18T18:16:19+00:00"}, {"path": "pytest_caplog-vs-syslog.md", "topic": "pytest", "title": "`caplog` vs `syslog` in pytest \u2013 What\u2019s the Difference?", "url": "https://github.com/CuriousLearner/til/blob/main/pytest/caplog-vs-syslog.md", "body": "When testing logging in `pytest`, you might come across `caplog` and `syslog`. Both capture logs, but they serve different purposes.\n\n## `caplog` \u2013 Captures Logs in pytest\nUse `caplog` to **intercept logs** emitted by your code and assert on them.\n\n```python\nimport logging\nimport pytest\n\ndef my_function():\n    logging.getLogger(\"myapp\").warning(\"Something went wrong!\")\n\ndef test_logging(caplog):\n    with caplog.at_level(logging.WARNING):\n        my_function()\n    assert \"Something went wrong!\" in caplog.text  # Captures log output\n```\n\n- **Use case:** Checking if a function logs expected messages.\n- **Works within pytest only** (not system-wide).\n\n## `syslog` \u2013 Captures System Logs\n`syslog` refers to **actual system-wide logs**, which pytest doesn't capture by default. If your app logs to system `syslog`, you need to **redirect it manually** for testing.\n\n```python\nimport subprocess\n\ndef test_syslog():\n    logs = subprocess.check_output([\"journalctl\", \"-n\", \"10\"]).decode()\n    assert \"CRITICAL ERROR\" not in logs  # Checks system logs\n```\n\n- **Use case:** Monitoring system logs (e.g., `/var/log/syslog`).\n- **Not pytest-specific** \u2013 It's for OS-level logging.\n\n## TL;DR\n- **Use `caplog` for pytest assertions** on logs inside Python.\n- **`syslog` is system-wide** and needs manual handling in tests.\n\nIf you're testing Python logs, **stick to `caplog`** \u2013 it's built for pytest!", "created": "2025-05-16T10:17:13+05:30", "created_utc": "2025-05-16T04:47:13+00:00", "updated": "2025-12-08T09:54:37+05:30", "updated_utc": "2025-12-08T04:24:37+00:00"}, {"path": "pytest_run-filtered-tests-by-substring.md", "topic": "pytest", "title": "Run tests that match substring in their name", "url": "https://github.com/CuriousLearner/til/blob/main/pytest/run-filtered-tests-by-substring.md", "body": "You can filter and run only tests that contain or do not contain some substring in their name.\n\nExamples:\n\n```bash\n# run all tests that contain `auth` in their name\n$ pytest -k auth\n\n# run all tests that do not contain `auth` in their name\n$ pytest -k 'not auth'\n```", "created": "2025-02-28T07:10:41+05:30", "created_utc": "2025-02-28T01:40:41+00:00", "updated": "2025-02-28T07:10:41+05:30", "updated_utc": "2025-02-28T01:40:41+00:00"}, {"path": "cli_pass-arguments-from-previous-command.md", "topic": "cli", "title": "Pass Arguments From Previous Command", "url": "https://github.com/CuriousLearner/til/blob/main/cli/pass-arguments-from-previous-command.md", "body": "`$_` can be used to pass arguments from previous command\n\nExample:\n\n```bash\nmkdir hello-go && code $_\n```", "created": "2023-07-19T17:56:42+05:30", "created_utc": "2023-07-19T12:26:42+00:00", "updated": "2025-02-28T07:22:58+05:30", "updated_utc": "2025-02-28T01:52:58+00:00"}, {"path": "cli_getent-passwd.md", "topic": "cli", "title": "Using `getent passwd` to query user accounts", "url": "https://github.com/CuriousLearner/til/blob/main/cli/getent-passwd.md", "body": "`getent passwd` queries the Name Service Switch (NSS) databases for user account information. Unlike directly reading `/etc/passwd`, it includes users from all configured sources (local files, LDAP, NIS, etc.).\n\n## Basic usage\n\n```bash\n# List all users\ngetent passwd\n\n# Query specific user\ngetent passwd sanyam\n```\n\n## Output format\n\nEach line contains 7 colon-separated fields:\n\n```\nusername:password:UID:GID:GECOS:home_directory:shell\n```\n\nExample output:\n\n```\nsanyam:x:1000:1000:Sanyam Khurana:/home/sanyam:/bin/bash\n```\n\n| Field          | Value            | Description                                         |\n| -------------- | ---------------- | --------------------------------------------------- |\n| username       | `sanyam`         | Login name                                          |\n| password       | `x`              | Password placeholder (actual hash in `/etc/shadow`) |\n| UID            | `1000`           | User ID                                             |\n| GID            | `1000`           | Primary group ID                                    |\n| GECOS          | `Sanyam Khurana` | Full name/comment field                             |\n| home_directory | `/home/sanyam`   | User's home directory                               |\n| shell          | `/bin/bash`      | Default login shell                                 |\n\n## Why use `getent` over `cat /etc/passwd`\n\n```bash\n# Only shows local users\ncat /etc/passwd\n\n# Shows ALL users (local + LDAP + NIS + other NSS sources)\ngetent passwd\n```\n\nThis distinction matters in enterprise environments where users may be centrally managed.", "created": "2026-01-25T04:49:34+05:30", "created_utc": "2026-01-24T23:19:34+00:00", "updated": "2026-01-25T04:49:34+05:30", "updated_utc": "2026-01-24T23:19:34+00:00"}, {"path": "cli_netcat-and-raw-tcp-connection.md", "topic": "cli", "title": "Netcat (`nc`) and Raw TCP Communication", "url": "https://github.com/CuriousLearner/til/blob/main/cli/netcat-and-raw-tcp-connection.md", "body": "**Netcat (`nc`)** is a powerful tool for working with raw TCP and UDP connections. Unlike higher-level tools like `curl` or `telnet`, `nc` communicates directly with a port without adding protocol-specific overhead, making it ideal for debugging low-level network issues.\n\n---\n\n### Using `nc` for Raw TCP Connections\n\nA basic connection test:\n\n```bash\nnc -v 127.0.0.1 9092\n```\n\n- `-v` (verbose) shows connection success or failure.\n- If a service (like Kafka) is running on `9092`, `nc` connects and waits for data.\n\n---\n\n### Sending Raw Data Over TCP\n\nNetcat lets you send arbitrary data:\n\n```bash\necho \"Hello Server\" | nc 127.0.0.1 9092\n```\n\n- Sends \"Hello Server\" over raw TCP to port `9092`.\n- If the server expects a specific protocol (e.g., Kafka), it may not respond, but the connection still works.\n\n---\n\n### Why Netcat?\n\n- Works with **raw TCP/UDP** (no HTTP, SSH, or extra formatting).\n- Can act as a **server**:\n\n  ```bash\n  nc -l 9092\n  ```\n\n  - This makes Netcat **listen** on port `9092`, waiting for incoming connections.\n  - Any data sent to this port by a client will be displayed in the terminal.\n- Great for **port scanning**:\n\n  ```bash\n  nc -zv 127.0.0.1 9092\n  ```\n\n  (The `-z` option checks if the port is open without sending data.)\n\nNetcat is a versatile network Swiss Army knife, especially useful for debugging services like **Kafka, Redis, or raw socket servers**!", "created": "2025-03-01T05:32:11+05:30", "created_utc": "2025-03-01T00:02:11+00:00", "updated": "2025-12-08T09:54:37+05:30", "updated_utc": "2025-12-08T04:24:37+00:00"}, {"path": "cli_useradd-with-expiration.md", "topic": "cli", "title": "Creating user accounts with expiration dates", "url": "https://github.com/CuriousLearner/til/blob/main/cli/useradd-with-expiration.md", "body": "Use `useradd -e` to create accounts that automatically expire on a specific date. Useful for temporary contractors, interns, or time-limited access.\n\n## Create user with expiration\n\n```bash\nsudo useradd -m -e 2027-03-28 rose\n```\n\n| Flag | Description |\n| ---- | ----------- |\n| `-m` | Create home directory |\n| `-e` | Account expiration date (YYYY-MM-DD format) |\n\n## Verify expiration date\n\n```bash\nsudo chage -l rose | grep -i \"Account expires\"\n# Output: Account expires : Mar 28, 2027\n```\n\nOr view all account aging info:\n\n```bash\nsudo chage -l rose\n```\n\n## Modify expiration on existing user\n\n```bash\n# Change expiration date\nsudo chage -E 2027-06-30 rose\n\n# Remove expiration (never expires)\nsudo chage -E -1 rose\n```\n\n## Related: password expiration vs account expiration\n\n- **Account expiration** (`-e`/`-E`): User cannot log in at all after this date\n- **Password expiration** (`-M`): User must change password after N days, but can still log in", "created": "2026-01-25T23:32:52+05:30", "created_utc": "2026-01-25T18:02:52+00:00", "updated": "2026-01-25T23:32:52+05:30", "updated_utc": "2026-01-25T18:02:52+00:00"}, {"path": "cli_apt-get-distclean.md", "topic": "cli", "title": "`apt-get distclean` for complete package cleanup", "url": "https://github.com/CuriousLearner/til/blob/main/cli/apt-get-distclean.md", "body": "`apt-get distclean` is a comprehensive cleanup command that removes downloaded package files, partial packages, and cleans up the package cache more thoroughly than `apt-get clean`.\n\nWhile `apt-get clean` only removes downloaded package files from the cache, `distclean` goes further by also removing partial packages and any other cached data that might be left behind.\n\n## Usage:\n\n```bash\nsudo apt-get distclean\n```\n\n## What it does:\n\n- Removes all downloaded package files from `/var/cache/apt/archives/`\n- Removes partial package files from `/var/cache/apt/archives/partial/`\n- Cleans up any other cached package data\n- Frees up more disk space than `apt-get clean`\n\n## Example:\n\n```bash\n# Check current cache size\ndu -sh /var/cache/apt/archives/\n\n# Perform complete cleanup\nsudo apt-get distclean\n\n# Verify cleanup\ndu -sh /var/cache/apt/archives/\n```\n\nThis is particularly useful when you need to free up maximum disk space or ensure a completely clean package cache state.", "created": "2025-09-12T01:24:48+05:30", "created_utc": "2025-09-11T19:54:48+00:00", "updated": "2025-09-12T01:24:48+05:30", "updated_utc": "2025-09-11T19:54:48+00:00"}, {"path": "cli_execute-previous-command-with-sudo.md", "topic": "cli", "title": "Execute last command with sudo", "url": "https://github.com/CuriousLearner/til/blob/main/cli/execute-previous-command-with-sudo.md", "body": "From the bash manual:\n\n> 9.3.1 Event Designators\n>\n> !! - Refer to the previous command. This is a synonym for \u2018!-1\u2019.\n\nUse `sudo !!` to run last command with uplifted privilege.", "created": "2023-02-13T22:37:47+05:30", "created_utc": "2023-02-13T17:07:47+00:00", "updated": "2023-02-13T22:37:47+05:30", "updated_utc": "2023-02-13T17:07:47+00:00"}, {"path": "cli_unzip-overwrite-specify-directory.md", "topic": "cli", "title": "Unzip overwrite and place content in a directory", "url": "https://github.com/CuriousLearner/til/blob/main/cli/unzip-overwrite-specify-directory.md", "body": "This command will unzip a file and overwrite any existing files with the same name. The `-o` flag is used to overwrite existing files, and the `-d` flag is used to specify the output directory.\n\n```bash\nunzip -o file.zip -d output_directory\n```", "created": "2025-02-10T22:43:20+05:30", "created_utc": "2025-02-10T17:13:20+00:00", "updated": "2025-02-10T22:43:20+05:30", "updated_utc": "2025-02-10T17:13:20+00:00"}, {"path": "cli_envsubst-command-for-config-files.md", "topic": "cli", "title": "`envsubst` command for substituting values in config files", "url": "https://github.com/CuriousLearner/til/blob/main/cli/envsubst-command-for-config-files.md", "body": "We can have files that contain variables which are replaced by environment variables at runtime. This is particularly useful for configuration files, where sensitive information like API keys or database passwords should not be hardcoded and checked into version control.\n\n`envsubst` is a command-line utility used for substituting the values of environment variables into strings or files. It's particularly useful for templating configuration files with environment-specific values.\n\n## Example:\n\nSuppose you have a configuration template file `config.template` with the following content:\n\n```bash\napi_url=http://$API_HOST:$API_PORT/api\ndb_url=postgres://$DB_USER:$DB_PASS@$DB_HOST:$DB_PORT/$DB_NAME\n```\n\nDefine env variables:\n\n```bash\nexport API_HOST=api.example.com\nexport API_PORT=8080\nexport DB_USER=myuser\nexport DB_PASS=mypassword\nexport DB_HOST=db.example.com\nexport DB_PORT=5432\nexport DB_NAME=mydatabase\n```\n\nRun `envsubst` to replace the placeholders with the actual values:\n\n```bash\nenvsubst < config.template > config.conf\n```\n\nThis will create a new file `config.conf` with the environment variables replaced.\n\nThe `config.conf` will now look like this:\n\n```bash\napi_url=http://api.example.com:8080/api\ndb_url=postgres://myuser:mypassword@db.example.com:5432/mydatabase\n```", "created": "2024-07-28T05:21:36+05:30", "created_utc": "2024-07-27T23:51:36+00:00", "updated": "2024-07-28T05:21:36+05:30", "updated_utc": "2024-07-27T23:51:36+00:00"}, {"path": "cli_pkill-with-full-pattern-matching.md", "topic": "cli", "title": "Using pkill with -f flag for full command line matching", "url": "https://github.com/CuriousLearner/til/blob/main/cli/pkill-with-full-pattern-matching.md", "body": "The `-f` flag in `pkill` matches against the entire command line, not just the process name.\n\n## Basic pkill vs pkill -f\n\n```bash\n# Only matches process name \"python\"\npkill python\n\n# Matches full command line containing \"manage.py runserver\"\npkill -f \"manage.py runserver\"\n```\n\n## Why it's useful for Django development\n\nWhen you run Django's development server:\n```bash\npython manage.py runserver\n```\n\nThe actual process name is `python`, not `runserver`. Without `-f`, you'd kill all Python processes:\n\n```bash\n# Dangerous - kills ALL python processes\npkill python\n\n# Safe - only kills Django runserver\npkill -f \"manage.py runserver\"\n```\n\n## How -f works\n\nThe `-f` flag tells `pkill` to match the pattern against the full command line shown in `ps aux`:\n\n```bash\n# See what would be matched\nps aux | grep \"manage.py runserver\"\n\n# Kill those specific processes\npkill -f \"manage.py runserver\"\n```\n\n## Other useful flags\n\n```bash\n# Check what would be killed (dry run)\npgrep -f \"manage.py runserver\"\n\n# Kill with specific signal\npkill -f -TERM \"manage.py runserver\"\n\n# Case insensitive matching\npkill -f -i \"MANAGE.py runserver\"\n```\n\n## Common use cases\n\n```bash\n# Kill specific Django port\npkill -f \"runserver 0.0.0.0:8000\"\n\n# Kill Celery workers\npkill -f \"celery worker\"\n\n# Kill specific pytest runs\npkill -f \"pytest tests/unit\"\n\n# Kill Jupyter notebooks\npkill -f \"jupyter-notebook\"\n```", "created": "2025-09-22T06:21:01+05:30", "created_utc": "2025-09-22T00:51:01+00:00", "updated": "2025-09-22T06:21:01+05:30", "updated_utc": "2025-09-22T00:51:01+00:00"}, {"path": "github_show-git-blame-for-a-file.md", "topic": "github", "title": "See git-blame for a file", "url": "https://github.com/CuriousLearner/til/blob/main/github/show-git-blame-for-a-file.md", "body": "While browsing the file on GitHub, if you press `b`, it will open up git-blame of that file, where you can see all changes in the file by everyone.", "created": "2023-02-13T03:47:46+05:30", "created_utc": "2023-02-12T22:17:46+00:00", "updated": "2023-02-13T03:47:46+05:30", "updated_utc": "2023-02-12T22:17:46+00:00"}, {"path": "github_add-funding-info.md", "topic": "github", "title": "Add Funding Information to GitHub Projects", "url": "https://github.com/CuriousLearner/til/blob/main/github/add-funding-info.md", "body": "GitHub allows you to display a \"Sponsor\" button on your repository by creating a `.github/FUNDING.yml` file. This makes it easy for users to support your open-source work.\n\n## Setup\n\nCreate a `.github/FUNDING.yml` file in your repository root:\n\n```yaml\n# .github/FUNDING.yml\ngithub: YourGitHubUsername\npatreon: YourPatreonUsername\nopen_collective: your-project\nko_fi: YourKoFiUsername\ntidelift: npm/package-name\ncommunity_bridge: project-name\nliberapay: YourLiberapayUsername\nissuehunt: YourIssueHuntUsername\notechie: YourOtechieUsername\nlfx_crowdfunding: project-name\ncustom: [\"https://your-website.com/donate\", \"https://example.com\"]\nbuy_me_a_coffee: YourBuyMeACoffeeUsername\n```\n\n## Supported platforms\n\nGitHub supports multiple funding platforms:\n- GitHub Sponsors (`github`)\n- Patreon (`patreon`)\n- Open Collective (`open_collective`)\n- Ko-fi (`ko_fi`)\n- Tidelift (`tidelift`)\n- Community Bridge (`community_bridge`)\n- Liberapay (`liberapay`)\n- IssueHunt (`issuehunt`)\n- Otechie (`otechie`)\n- LFX Crowdfunding (`lfx_crowdfunding`)\n- Buy Me a Coffee (`buy_me_a_coffee`)\n- Custom URLs (`custom`)\n\n## Example\n\n```yaml\n# .github/FUNDING.yml\ngithub: CuriousLearner\nbuy_me_a_coffee: CuriousLearner\n```\n\nOnce committed, GitHub will automatically display a \"Sponsor\" button on your repository's main page with links to your funding platforms.\n\n## Important notes\n\n**File location**: The file must be named exactly `FUNDING.yml` (or `FUNDING.yaml`) and placed in the `.github` directory at the repository root.\n\n**Visibility**: The sponsor button appears on public repositories. It won't show on private repositories.\n\n**Multiple accounts**: You can specify multiple usernames for platforms that support it (like GitHub Sponsors) by using an array:\n\n```yaml\ngithub: [user1, user2, user3]\n```\n\n**Custom URLs**: Use the `custom` field for any donation link not covered by the supported platforms (maximum of 4 custom URLs).\n\nUse GitHub funding configuration to make it easy for the community to support your open-source contributions.", "created": "2025-10-21T09:13:54+05:30", "created_utc": "2025-10-21T03:43:54+00:00", "updated": "2025-10-21T09:13:54+05:30", "updated_utc": "2025-10-21T03:43:54+00:00"}, {"path": "pdf_compress-pdf.md", "topic": "pdf", "title": "Reduce file size of PDF files", "url": "https://github.com/CuriousLearner/til/blob/main/pdf/compress-pdf.md", "body": "This requires `gs` (Ghostscript) to be installed.\n\n```bash\ncompresspdf() {\n    if [ $# -lt 2 ]; then\n        echo \"Usage: compresspdf INPUT.pdf OUTPUT.pdf [dpi]\"\n        echo \"Default dpi = 200 (good for scanned docs / visa uploads)\"\n        return 1\n    fi\n\n    local input=\"$1\"\n    local output=\"$2\"\n    local dpi=\"${3:-200}\"   # third arg optional, default 200\n\n    command gs \\\n      -sDEVICE=pdfwrite \\\n      -dNOPAUSE -dQUIET -dBATCH \\\n      -dCompatibilityLevel=1.6 \\\n      -dDetectDuplicateImages=true \\\n      -dDownsampleColorImages=true \\\n      -dColorImageDownsampleType=/Average \\\n      -dColorImageResolution=\"$dpi\" \\\n      -dDownsampleGrayImages=true \\\n      -dGrayImageDownsampleType=/Average \\\n      -dGrayImageResolution=\"$dpi\" \\\n      -dDownsampleMonoImages=true \\\n      -dMonoImageDownsampleType=/Average \\\n      -dMonoImageResolution=300 \\\n      -sOutputFile=\"$output\" \\\n      \"$input\"\n}\n```\n\n## Usage\n\n```bash\n# Default 200 DPI (good for scanned docs, visa uploads)\ncompresspdf input.pdf output.pdf\n\n# Custom DPI for higher quality\ncompresspdf input.pdf output.pdf 300\n```\n\n## What each option does\n\n| Option                                | Description                                                       |\n| ------------------------------------- | ----------------------------------------------------------------- |\n| `-sDEVICE=pdfwrite`                   | Output device - writes a new PDF file                             |\n| `-dNOPAUSE`                           | Don't pause between pages (normally gs waits for input)           |\n| `-dQUIET`                             | Suppress startup messages and page numbers                        |\n| `-dBATCH`                             | Exit after processing (don't enter interactive mode)              |\n| `-dCompatibilityLevel=1.6`            | PDF version 1.6 - supports better compression than older versions |\n| `-dDetectDuplicateImages=true`        | Find identical images and store them only once                    |\n| `-dDownsampleColorImages=true`        | Reduce resolution of color images                                 |\n| `-dColorImageDownsampleType=/Average` | Use averaging when reducing (smoother than `/Subsample`)          |\n| `-dColorImageResolution=<dpi>`        | Target DPI for color images                                       |\n| `-dDownsampleGrayImages=true`         | Reduce resolution of grayscale images                             |\n| `-dGrayImageDownsampleType=/Average`  | Averaging for grayscale (smoother results)                        |\n| `-dGrayImageResolution=<dpi>`         | Target DPI for grayscale images                                   |\n| `-dDownsampleMonoImages=true`         | Reduce resolution of black & white images                         |\n| `-dMonoImageDownsampleType=/Average`  | Averaging for mono images                                         |\n| `-dMonoImageResolution=300`           | Keep mono at 300 DPI (text/line art needs more resolution)        |\n| `-sOutputFile=<file>`                 | Output file path                                                  |\n\n## Why 200 DPI default?\n\n200 DPI is enough for on-screen viewing and most document uploads (visa applications, forms, etc.) while significantly reducing file size. Increase to 300 DPI if you need to print the document.", "created": "2023-03-15T04:02:40+05:30", "created_utc": "2023-03-14T22:32:40+00:00", "updated": "2025-11-28T06:21:00+05:30", "updated_utc": "2025-11-28T00:51:00+00:00"}, {"path": "pdf_remove-pdf-password.md", "topic": "pdf", "title": "Remove PDF Password", "url": "https://github.com/CuriousLearner/til/blob/main/pdf/remove-pdf-password.md", "body": "Install `qpdf` via brew like:\n\n```bash\nbrew install qpdf\n```\n\nand then run:\n\n```bash\nqpdf --decrypt --password=xxxxx encrypted-filename.pdf decrypted-filename.pdf\n```", "created": "2023-03-15T04:02:40+05:30", "created_utc": "2023-03-14T22:32:40+00:00", "updated": "2025-02-28T07:22:58+05:30", "updated_utc": "2025-02-28T01:52:58+00:00"}, {"path": "html_server-sent-events.md", "topic": "html", "title": "Server-Sent Events (SSE)", "url": "https://github.com/CuriousLearner/til/blob/main/html/server-sent-events.md", "body": "Server-Sent Events is an HTTP-based protocol for pushing real-time updates from server to client over a single, long-lived connection. Unlike WebSockets, SSE is one-directional (server \u2192 client), uses plain HTTP, and works through proxies and load balancers without special configuration.\n\n## How it works\n\n1. The client opens a connection using the `EventSource` API\n2. The server responds with `Content-Type: text/event-stream` and keeps the connection open\n3. The server sends messages as plain text frames whenever it has new data\n4. If the connection drops, the browser automatically reconnects\n\n## Message format\n\nEach message is a block of `field: value` lines, separated by a blank line:\n\n```\ndata: Hello world\n\ndata: {\"user\": \"alice\", \"message\": \"hi\"}\n\nevent: status_update\ndata: {\"online\": 42}\n\nid: 15\ndata: This message has an ID for resuming\n```\n\n- `data:` \u2014 the payload (required)\n- `event:` \u2014 custom event type (default is `message`)\n- `id:` \u2014 sets the last event ID; on reconnect the browser sends `Last-Event-ID` header so the server can resume from where it left off\n- `retry:` \u2014 tells the browser how many milliseconds to wait before reconnecting\n\n## Client side (browser)\n\n```javascript\nconst source = new EventSource(\"/events\");\n\n// Default \"message\" events\nsource.addEventListener(\"message\", (e) => {\n    console.log(e.data);\n});\n\n// Custom named events\nsource.addEventListener(\"status_update\", (e) => {\n    const data = JSON.parse(e.data);\n    console.log(`Online users: ${data.online}`);\n});\n\nsource.addEventListener(\"error\", (e) => {\n    console.log(\"Connection lost, browser will auto-reconnect\");\n});\n```\n\n## Server side (Python example)\n\n```python\nfrom flask import Flask, Response\nimport time, json\n\napp = Flask(__name__)\n\n@app.route('/events')\ndef events():\n    def stream():\n        while True:\n            data = json.dumps({\"time\": time.time()})\n            yield f\"data: {data}\\n\\n\"\n            time.sleep(1)\n\n    return Response(stream(), content_type='text/event-stream',\n                    headers={'Cache-Control': 'no-cache',\n                             'X-Accel-Buffering': 'no'})\n```\n\nThe `X-Accel-Buffering: no` header tells Nginx not to buffer the response, which is important when running behind a reverse proxy.\n\n## SSE vs WebSockets\n\n|                         | SSE                          | WebSockets             |\n| ----------------------- | ---------------------------- | ---------------------- |\n| Direction               | Server \u2192 Client              | Bidirectional          |\n| Protocol                | HTTP                         | `ws://` / `wss://`     |\n| Reconnection            | Automatic (built-in)         | Manual                 |\n| Data format             | Text only                    | Text and binary        |\n| Resume support          | Built-in via `Last-Event-ID` | Manual                 |\n| Proxy/firewall friendly | Yes (plain HTTP)             | Sometimes needs config |\n| Browser support         | All modern browsers          | All modern browsers    |\n\n## Common use cases\n\n- **LLM/AI chat streaming** \u2014 streaming token-by-token responses (this is how ChatGPT, Claude, and most LLM APIs stream responses)\n- **Live notifications** \u2014 new messages, alerts, system events\n- **Dashboards** \u2014 real-time metrics, stock tickers, scores\n- **Progress updates** \u2014 file uploads, long-running jobs, build pipelines\n- **Live feeds** \u2014 news, social media timelines, log tailing\n\n## When to pick SSE over WebSockets\n\nChoose SSE when data only flows server => client. It's simpler to implement, works over standard HTTP infrastructure, handles reconnection and resume automatically, and doesn't require a separate protocol upgrade. Use WebSockets when you need bidirectional communication (e.g., multiplayer games, collaborative editing).", "created": "2026-02-26T11:52:52+05:30", "created_utc": "2026-02-26T06:22:52+00:00", "updated": "2026-02-26T11:52:52+05:30", "updated_utc": "2026-02-26T06:22:52+00:00"}, {"path": "html_sri-hashes.md", "topic": "html", "title": "Subresource Integrity (SRI) Hashes", "url": "https://github.com/CuriousLearner/til/blob/main/html/sri-hashes.md", "body": "SRI is a security feature that allows browsers to verify that resources fetched from CDNs or external sources haven't been tampered with. It works by comparing a cryptographic hash of the fetched resource against an expected hash you provide.\n\n## Usage\n\nAdd the `integrity` attribute to `<script>` or `<link>` tags:\n\n```html\n<script src=\"https://cdn.example.com/library.js\"\n        integrity=\"sha384-oqVuAfXRKap7fdgcCY5uykM6+R9GqQ8K/uxy9rx7HNQlGYl1kPzQho1wx4JwY8wC\"\n        crossorigin=\"anonymous\"></script>\n\n<link rel=\"stylesheet\" href=\"https://cdn.example.com/styles.css\"\n      integrity=\"sha384-abc123...\"\n      crossorigin=\"anonymous\">\n```\n\nThe `crossorigin=\"anonymous\"` attribute is required when loading resources from a different origin.\n\n## Generating SRI Hashes\n\nUsing OpenSSL:\n\n```bash\ncat library.js | openssl dgst -sha384 -binary | openssl base64 -A\n```\n\nOr using shasum:\n\n```bash\nshasum -b -a 384 library.js | awk '{ print $1 }' | xxd -r -p | base64\n```\n\nYou can also use online tools like [srihash.org](https://www.srihash.org/).\n\n## Supported Hash Algorithms\n\nSRI supports SHA-256, SHA-384, and SHA-512. SHA-384 is commonly used as a good balance between security and performance.\n\nYou can specify multiple hashes for the same resource (browser uses the strongest one it supports):\n\n```html\n<script src=\"https://cdn.example.com/lib.js\"\n        integrity=\"sha256-abc... sha384-xyz...\"\n        crossorigin=\"anonymous\"></script>\n```\n\n## Why Use SRI?\n\n- Protects against compromised CDNs or man-in-the-middle attacks\n- Ensures the exact version of a library you tested is what users receive\n- Prevents malicious code injection through third-party resources", "created": "2026-01-07T09:06:31+05:30", "created_utc": "2026-01-07T03:36:31+00:00", "updated": "2026-01-07T09:06:31+05:30", "updated_utc": "2026-01-07T03:36:31+00:00"}, {"path": "django_empty-database.md", "topic": "django", "title": "Empty database", "url": "https://github.com/CuriousLearner/til/blob/main/django/empty-database.md", "body": "`python manage.py flush` to empty all tables in the database.\n\n`python manage.py reset_db` to drop the database schema. Particularly helpful when trying to restore a database dump.", "created": "2023-07-08T01:00:47+05:30", "created_utc": "2023-07-07T19:30:47+00:00", "updated": "2023-07-08T01:00:47+05:30", "updated_utc": "2023-07-07T19:30:47+00:00"}, {"path": "django_queryset-explain-analyze.md", "topic": "django", "title": "Using EXPLAIN and EXPLAIN ANALYZE for Django QuerySets", "url": "https://github.com/CuriousLearner/til/blob/main/django/queryset-explain-analyze.md", "body": "Django provides `.explain()` to analyze query execution plans and identify performance bottlenecks.\n\n## Basic usage\n\n```python\n# EXPLAIN - Shows query plan without execution\nprint(Article.objects.filter(status='published').explain())\n\n# EXPLAIN ANALYZE - Actually runs the query and shows real timing\nprint(Article.objects.filter(status='published').explain(analyze=True))\n```\n\n## Reading the results\n\nThe output varies by database, but here's what to look for in PostgreSQL:\n\n```\nSeq Scan on article  (cost=0.00..35.50 rows=10 width=100) (actual time=0.012..0.156 rows=8 loops=1)\n  Filter: (status = 'published'::text)\n  Rows Removed by Filter: 2\nPlanning Time: 0.084 ms\nExecution Time: 0.198 ms\n```\n\n### Key metrics\n\n**cost=0.00..35.50**: Estimated startup cost..total cost (arbitrary units)\n- Lower is better\n- Used by query planner to choose between strategies\n\n**rows=10**: Estimated number of rows returned\n\n**actual time=0.012..0.156**: Real startup time..total time in milliseconds (only with `analyze=True`)\n\n**rows=8**: Actual rows returned (only with `analyze=True`)\n\n**Planning Time / Execution Time**: Total time breakdown\n\n## Identifying bottlenecks\n\n### Sequential scans on large tables\n```\nSeq Scan on article  (cost=0.00..10000.00 rows=500000 width=100)\n```\n**Problem**: Full table scan instead of using an index\n**Solution**: Add an index on the filtered column\n\n### Missing index usage\n```python\n# Check if an index is being used\nprint(Article.objects.filter(created_at__gte=date).explain())\n# Look for \"Index Scan\" or \"Index Only Scan\" instead of \"Seq Scan\"\n```\n\n### Nested loops on large datasets\n```\nNested Loop  (cost=0.00..50000.00 rows=10000 width=200)\n```\n**Problem**: Inefficient join strategy\n**Solution**: Use `select_related()` or check if statistics are outdated (run `ANALYZE` on tables)\n\n### High actual vs estimated rows\n```\nHash Join  (cost=10.00..20.00 rows=100 width=50) (actual rows=50000 loops=1)\n```\n**Problem**: Query planner has outdated statistics\n**Solution**: Run `VACUUM ANALYZE` on the table\n\n## Database-specific options\n\n```python\n# PostgreSQL: verbose output with buffer usage\nArticle.objects.all().explain(verbose=True, analyze=True, buffers=True)\n\n# PostgreSQL: different output formats\nArticle.objects.all().explain(format='json', analyze=True)\n```\n\n## Best practices\n\n1. **Always use `analyze=True` for real performance data** - estimated costs can be misleading\n2. **Test with production-like data volumes** - query plans change with table size\n3. **Look for sequential scans** on large tables first - usually the biggest wins\n4. **Check actual time** at each node to find the slowest operation\n5. **Compare estimated vs actual rows** - large differences indicate stale statistics", "created": "2025-10-03T04:09:11+05:30", "created_utc": "2025-10-02T22:39:11+00:00", "updated": "2025-10-03T04:09:11+05:30", "updated_utc": "2025-10-02T22:39:11+00:00"}, {"path": "django_add-q-in-django-queryset.md", "topic": "django", "title": "Using `add_q` in Django Queries", "url": "https://github.com/CuriousLearner/til/blob/main/django/add-q-in-django-queryset.md", "body": "In Django, the `add_q` method is part of the internal QuerySet API and is used to combine complex query conditions using Q objects. It allows you to dynamically build and combine queries with logical operators.\n\n```python\nfrom django.db.models import Q\nfrom myapp.models import MyModel\n\n# Create an initial queryset\nqs = MyModel.objects.all()\n\n# Create Q objects for complex conditions\nq1 = Q(name__icontains='example')\nq2 = Q(age__gte=18) | Q(city='New York')\n\n# Use add_q to combine conditions\nqs.query.add_q(q1)\nqs.query.add_q(q2)\n\n# Evaluate the final queryset\nresults = qs.all()\n```\n\nThis will be an implicit `AND` between `q1` and `q2`, meaning it will return rows where both conditions are true. You can also explicitly use `Q.And` for an `AND` or `Q.Or` for an `OR`.", "created": "2024-06-23T09:36:21+05:30", "created_utc": "2024-06-23T04:06:21+00:00", "updated": "2024-06-23T09:36:21+05:30", "updated_utc": "2024-06-23T04:06:21+00:00"}, {"path": "django_foreign-key-attrs.md", "topic": "django", "title": "Understanding Django ForeignKey Fields", "url": "https://github.com/CuriousLearner/til/blob/main/django/foreign-key-attrs.md", "body": "Django\u2019s `ForeignKey` has several useful options that control how relationships behave. Here are some you should know:\n\n```python\nclass Invoice(models.Model):\n    customer = models.ForeignKey(\n        \"Customer\",\n        on_delete=models.CASCADE,  # Deletes Invoice if Customer is deleted\n        to_field=\"customer_code\",  # Links to a non-PK field\n        db_column=\"customer_id\",   # Custom column name in the DB\n        related_name=\"invoices\",   # Enables reverse lookup: customer.invoices.all()\n        db_constraint=True,        # Enforces FK constraint at the DB level\n    )\n```\n\n\ud83d\udd39 `on_delete=models.CASCADE` \u2192 Deletes invoices when the customer is deleted.\n\ud83d\udd39 `to_field=\"customer_code\"` \u2192 Uses `customer_code` instead of the default `id`.\n\ud83d\udd39 `db_column=\"customer_id\"` \u2192 Maps the field to a specific column in SQL.\n\ud83d\udd39 `related_name=\"invoices\"` \u2192 Allows reverse access like `customer.invoices.all()`.\n\ud83d\udd39 `db_constraint=False` \u2192 Removes DB-level constraints, useful for external references.\n\nWant a `ForeignKey` without constraints? Set `db_constraint=False`, but be mindful of orphaned records.", "created": "2025-05-16T10:17:13+05:30", "created_utc": "2025-05-16T04:47:13+00:00", "updated": "2025-05-16T10:17:13+05:30", "updated_utc": "2025-05-16T04:47:13+00:00"}, {"path": "django_order-by-random.md", "topic": "django", "title": "Random ordering with order_by(\"?\") in Django", "url": "https://github.com/CuriousLearner/til/blob/main/django/order-by-random.md", "body": "Django provides a convenient way to retrieve records in random order using `order_by(\"?\")`:\n\n```python\n# Get 5 random articles\nrandom_articles = Article.objects.order_by(\"?\")[:5]\n```\n\nThis uses the database's random function to shuffle results:\n- PostgreSQL: `RANDOM()`\n- MySQL: `RAND()`\n- SQLite: `RANDOM()`\n- Oracle: `DBMS_RANDOM.VALUE()`\n\n## Important considerations\n\n**Performance**: Random ordering can be slow on large tables since it requires the database to generate a random value for each row and then sort them. For better performance on large datasets, consider these alternatives:\n\n```python\nimport random\n\n# Option 1: Random offset (works for small-ish result sets)\ncount = Article.objects.published().count()\nrandom_offset = random.randint(0, max(0, count - 20))\narticles = Article.objects.published()[random_offset:random_offset + 20]\n\n# Option 2: Random IDs (better for large sets)\npublished_ids = list(Article.objects.published().values_list('id', flat=True))\nrandom_ids = random.sample(published_ids, min(20, len(published_ids)))\narticles = Article.objects.filter(id__in=random_ids)\n\n# Option 3: Cache random IDs (best for hot paths)\n# Cache the full ID list, sample from cache to avoid DB queries\n```\n\n**Consistency**: Each call to `order_by(\"?\")` produces a different random order, so pagination won't work reliably with random ordering.\n\nUse `order_by(\"?\")` for small datasets or when you need truly random results and performance isn't critical.", "created": "2025-10-02T20:46:53+05:30", "created_utc": "2025-10-02T15:16:53+00:00", "updated": "2025-10-02T20:46:53+05:30", "updated_utc": "2025-10-02T15:16:53+00:00"}, {"path": "django_isolate-redis-cache-for-parallel-tests.md", "topic": "django", "title": "Isolate Redis cache for parallel tests", "url": "https://github.com/CuriousLearner/til/blob/main/django/isolate-redis-cache-for-parallel-tests.md", "body": "When running Django tests in parallel, the default Redis cache can cause issues. To avoid conflicts, you can isolate the cache for each test worker.\n\nHere's how you can set up a unique cache for each worker:\n\n1. **Update `CACHES` in `settings/testing.py`**:\n\n```python\nif ENVIRONMENT not in [\"staging\", \"production\"]:\n    # Get unique Redis DB for each test worker\n    worker_id = os.getenv(\"DJANGO_TEST_PROCESSES\", \"1\")  # Django assigns a worker number\n    redis_db = int(worker_id) % 16  # Redis has databases 0-15 by default\n\n    CACHES[\"test\"] = {\n        \"BACKEND\": \"django_redis.cache.RedisCache\",\n        \"LOCATION\": f\"redis://redis:6379/{redis_db}\",  # Assign unique DB for parallel test isolation\n        \"OPTIONS\": {\n            \"CLIENT_CLASS\": \"django_redis.client.DefaultClient\",\n        },\n    }\n\n```\n\n2. **Run tests with `DJANGO_SETTINGS_MODULE=settings.testing`**:\n\n   ```bash\n    DJANGO_SETTINGS_MODULE=settings.testing ./manage.py test --parallel\n    ```\n\n`DJANGO_TEST_PROCESSES` is set by Django to assign a unique worker number for each test process. We use this to calculate a unique Redis database number for each worker.\n\nNow, each test worker will use a separate Redis cache, preventing conflicts during parallel test runs.", "created": "2025-02-20T20:54:13+05:30", "created_utc": "2025-02-20T15:24:13+00:00", "updated": "2025-12-08T09:54:37+05:30", "updated_utc": "2025-12-08T04:24:37+00:00"}, {"path": "django_generate-random-secrety-key.md", "topic": "django", "title": "Generate random secret key", "url": "https://github.com/CuriousLearner/til/blob/main/django/generate-random-secrety-key.md", "body": "```python\nfrom django.core.management.utils import get_random_secret_key\n\nsecret_key = get_random_secret_key()\nprint(secret_key)\n```\n\nThis generates a random secret key like `5gga#kdyy9twdab)v@_p%koib6mc_1qmpe8b-=5nm4lf24m-s)`", "created": "2024-06-03T20:02:52+05:30", "created_utc": "2024-06-03T14:32:52+00:00", "updated": "2025-02-27T00:08:04-05:00", "updated_utc": "2025-02-27T05:08:04+00:00"}, {"path": "django_separate-db-and-state-migration.md", "topic": "django", "title": "Managing Schema-less Migrations in Django with `SeparateDatabaseAndState`", "url": "https://github.com/CuriousLearner/til/blob/main/django/separate-db-and-state-migration.md", "body": "Ever needed to update Django\u2019s ORM model without altering the actual database schema? That\u2019s where `SeparateDatabaseAndState` comes in.\n\n## Use Case\n\nImagine you\u2019re working with an **external read-only reporting database** (`legacy_db`) where Django should track relationships but **shouldn\u2019t modify tables**. You have:\n\n```python\nclass Order(models.Model):\n    legacy_customer = models.CharField(max_length=255, db_column=\"customer_id\")\n```\n\nNow, you want to change this field into a `ForeignKey` to `LegacyCustomer` without actually modifying `legacy_db.customer`:\n\n```python\nclass Order(models.Model):\n    legacy_customer = models.ForeignKey(\n        \"legacy.LegacyCustomer\",\n        to_field=\"customer_id\",\n        db_column=\"customer_id\",\n        on_delete=models.DO_NOTHING,\n        db_constraint=False,\n    )\n```\n\nApplying this as a standard migration could fail or modify `legacy_db`, which we don\u2019t want. Instead, use:\n\n```python\nmigrations.SeparateDatabaseAndState(\n    state_operations=[\n        migrations.RemoveField(\"order\", \"legacy_customer\"),\n        migrations.AddField(\n            \"order\",\n            \"legacy_customer\",\n            models.ForeignKey(\n                \"legacy.LegacyCustomer\",\n                to_field=\"customer_id\",\n                db_column=\"customer_id\",\n                on_delete=models.DO_NOTHING,\n                db_constraint=False,\n            ),\n        ),\n    ]\n)\n```\n\nThis updates Django's ORM state **without touching the external DB**.", "created": "2025-05-16T10:17:13+05:30", "created_utc": "2025-05-16T04:47:13+00:00", "updated": "2025-12-08T09:54:37+05:30", "updated_utc": "2025-12-08T04:24:37+00:00"}, {"path": "django_coverage-on-parallel-tests.md", "topic": "django", "title": "Fixing Coverage for Django's `manage.py test --parallel`", "url": "https://github.com/CuriousLearner/til/blob/main/django/coverage-on-parallel-tests.md", "body": "Running `manage.py test --parallel` **dropped coverage** significantly, even though all tests passed. After debugging, I found the following key fixes:\n\n## 1. Enable Coverage for Subprocesses\n\nDjango **spawns subprocesses** for parallel tests, but coverage wasn\u2019t tracking them. To fix this, I enabled **`coverage.process_startup()`**, which ensures coverage runs in every spawned process:\n\n- **Manual Method (For Normal Projects)**:\n  Add this to `sitecustomize.py`:\n\n  ```python\n  import coverage\n  coverage.process_startup()\n  ```\n\n- **Better Alternative (What I Used)**:\n\n    Instead of modifying `sitecustomize.py`, install:\n\n    ```bash\n    pip install coverage_enable_subprocess\n    ```\n\n    This automatically enables coverage tracking in all subprocesses. This is specially useful for Django case when you won't to spend time in configuring `PYTHONPATH` and ensuring `sitecustomize` load before `django.load()` It took a while to figure this out.\n\n    Save yourself time, and install this.\n\n## 2. Properly Configuring `.coveragerc`\n\nI updated `.coveragerc` to explicitly enable multiprocessing:\n\n```\n[run]\nparallel = True\nconcurrency = multiprocessing\nsource = src\ndynamic_context = test_function\n```\n\n## 3. Fixing coverage combine Issues\n\nSince coverage runs separately in each parallel process, it generates multiple `.coverage` files. I had to explicitly combine them:\n\n```bash\ncoverage combine /tmp/coverage/\ncoverage report -m\ncoverage xml -o /tmp/coverage/coverage.xml\n```\n\n## 4. Exporting `COVERAGE_PROCESS_START`\n\nTo ensure subprocesses use the correct coverage config, I set:\n\n```bash\nexport COVERAGE_PROCESS_START=.coveragerc\n```\n\nThis ensures all spawned test processes follow `.coveragerc` settings.\n\n## 5. Removing `--append` from `coverage run`\n\nEarlier, `coverage run --append` caused conflicts in parallel mode. Instead, I now let coverage combine handle merging.\n\n## 6. Ensuring Coverage Files are Properly Named\n\nEach test run was generating `.coverage.<hostname>.<random_id>`, and combining them correctly was crucial. I confirmed they were stored in `/tmp/coverage/`.\n\n\n## Conclusion\n\nCombined examples from various sources to fix coverage issues with Django's parallel tests.\n\n```bash\nNPROC := $(shell if command -v nproc >/dev/null; then nproc; else sysctl -n hw.ncpu; fi)\nCOVERAGE_FILE=/tmp/coverage/.coverage.$(hostname).$$RANDOM \\\nCOVERAGE_PROCESS_START=.coveragerc \\\ncoverage run --source=src \\\nsrc/manage.py test --parallel $(NPROC) --keepdb --no-input && \\\ncoverage combine /tmp/coverage/ && \\\ncoverage report -m && \\\ncoverage xml -o /tmp/coverage/coverage.xml\n```\n\nNow, coverage remains stable even when running tests in parallel.", "created": "2025-02-20T07:07:26+05:30", "created_utc": "2025-02-20T01:37:26+00:00", "updated": "2025-12-08T09:54:37+05:30", "updated_utc": "2025-12-08T04:24:37+00:00"}, {"path": "django_redundant-all-in-queryset.md", "topic": "django", "title": "Redundant all chaining in queryset", "url": "https://github.com/CuriousLearner/til/blob/main/django/redundant-all-in-queryset.md", "body": "The default queryset from manager includes all the objects already. So, in most cases, `.all()` call can be avoided while chaining querysets.\n\nFor example:\n\n```python\nPost.objects.all().filter(title__startswith='Cool post')\n```\n\ncan be trimmed down to:\n\n```python\nPost.objects.filter(title__startswith='Cool post')\n```\n\nThe only reason to chain from `.all()` is for `delete()`, as a safeguard to ensure you really mean to delete everything: `Post.objects.all().delete()`.", "created": "2023-02-12T03:07:11+05:30", "created_utc": "2023-02-11T21:37:11+00:00", "updated": "2023-02-12T03:07:11+05:30", "updated_utc": "2023-02-11T21:37:11+00:00"}, {"path": "django_admin-list-select-related.md", "topic": "django", "title": "Speed Up Django Admin with `list_select_related`", "url": "https://github.com/CuriousLearner/til/blob/main/django/admin-list-select-related.md", "body": "Django Admin can get **slow** when displaying related fields due to multiple database queries. Use `list_select_related` to **optimize performance** by fetching related data in a **single query**.\n\n## Example\n\nYou have an `Order` model with a foreign key to `Customer`:\n\n```python\nclass Order(models.Model):\n    customer = models.ForeignKey(\"Customer\", on_delete=models.CASCADE)\n    total_price = models.DecimalField(max_digits=10, decimal_places=2)\n```\n\nIn Django Admin:\n\n```python\nclass OrderAdmin(admin.ModelAdmin):\n    list_display = (\"id\", \"customer\", \"total_price\")\n    list_select_related = (\"customer\",)  # Pre-fetch related customer data in one query\n```\n\n## Why?\n- **Without `list_select_related`** - Each row triggers an **extra query** for `customer`.\n- **With `list_select_related`** - Uses **JOINs** to fetch all related data **at once**, reducing query count.\n\n**Pro Tip:** For **multiple ForeignKeys**, use `list_select_related = (\"customer\", \"sales_rep\")`.\n\n**Result:** Faster Django Admin with fewer queries!", "created": "2025-05-16T10:17:13+05:30", "created_utc": "2025-05-16T04:47:13+00:00", "updated": "2025-12-08T09:54:37+05:30", "updated_utc": "2025-12-08T04:24:37+00:00"}, {"path": "django_defer-fields-for-performance.md", "topic": "django", "title": "Use `defer()` to limit fields fetched from models", "url": "https://github.com/CuriousLearner/til/blob/main/django/defer-fields-for-performance.md", "body": "This is a use-case I came across in a project that uses PostGIS and manages lat/long as a `PointField`. If you retrieve this GIS data, when you don't need it, it has an expensive overhead of fetching all metadata of GIS. This can cause a serious bottleneck in your APIs.\n\nYou can use `defer` method to limit the fields fetched.\n\nFor example:\n\n```Python\nrestaurant_qs = Restaurant.objects.defer('point')\n```\n\nThis will fetch all the restaurants without the `point` field -- which means avoid loading all GIS data that isn't needed.\n\nA bit of warning though; if you end up accessing deferred field on any instance of queryset, it will make another trip to database to fetch that information.", "created": "2023-02-12T03:39:10+05:30", "created_utc": "2023-02-11T22:09:10+00:00", "updated": "2023-02-12T03:39:10+05:30", "updated_utc": "2023-02-11T22:09:10+00:00"}, {"path": "django_model-bakery-refresh-after-create.md", "topic": "django", "title": "Using `_refresh_after_create` in model_bakery", "url": "https://github.com/CuriousLearner/til/blob/main/django/model-bakery-refresh-after-create.md", "body": "When using [model_bakery](https://github.com/model-bakers/model_bakery) to create test fixtures that involve models with special behaviors like `DirtyFieldsMixin`, you might need to refresh the instance from the database after creation to ensure tracking mechanisms are properly initialized.\n\nInstead of manually calling `refresh_from_db()`:\n\n```python\nfrom model_bakery import baker\n\ndm = baker.make(\n    MyModel,\n    name=\"Original Name\",\n)\n# Refresh to ensure DirtyFieldsMixin tracking is reset after creation\ndm.refresh_from_db()\n```\n\nYou can use the `_refresh_after_create` parameter:\n\n```python\ndm = baker.make(\n    MyModel,\n    name=\"Original Name\",\n    _refresh_after_create=True,\n)\n```\n\nThis parameter automatically refreshes the instance from the database after creation, which is particularly useful when:\n- Working with models that use `DirtyFieldsMixin` or similar tracking mechanisms\n- Ensuring database-generated values (like auto-generated fields) are properly loaded\n- Testing models with custom save methods that modify data during creation\n\nThe `_refresh_after_create=True` approach is cleaner and more declarative, making the intent clear while reducing boilerplate code in your tests.", "created": "2025-09-23T22:25:31+05:30", "created_utc": "2025-09-23T16:55:31+00:00", "updated": "2025-09-25T22:28:37-04:00", "updated_utc": "2025-09-26T02:28:37+00:00"}, {"path": "django_set-whole-queryset-in-m2m-relationship.md", "topic": "django", "title": "Add Multiple Objects In Queryset In Many To Many Relationship", "url": "https://github.com/CuriousLearner/til/blob/main/django/set-whole-queryset-in-m2m-relationship.md", "body": "In Django, to add multiple objects in a many-to-many relationship, use the `set()` method. This replaces current related objects with a new set.\n\nFor example, with models Ad and Newsletter having a many-to-many relationship via newsletter:\n\n```python\nnew_ad.newsletter.set(original_ad.newsletter.all())\n```\n\n**NOTE**: This sets new_ad's newsletters to be the same as original_ad's. No need for `.save()` after `.set()`.", "created": "2024-06-23T09:12:54+05:30", "created_utc": "2024-06-23T03:42:54+00:00", "updated": "2024-06-23T09:12:54+05:30", "updated_utc": "2024-06-23T03:42:54+00:00"}, {"path": "django_migration-modules-setting.md", "topic": "django", "title": "Override Migration Locations with MIGRATION_MODULES in Django", "url": "https://github.com/CuriousLearner/til/blob/main/django/migration-modules-setting.md", "body": "Django's `MIGRATION_MODULES` setting allows you to override the default location where migrations are stored for specific apps. This is particularly useful when you need to extend or modify migrations for third-party packages without touching their source code in site-packages.\n\n```python\n# settings.py\nMIGRATION_MODULES = {\n    'myapp': 'myproject.db.migrations.myapp',\n    'third_party_app': 'myproject.db.migrations.third_party_app',\n}\n```\n\n## Common use cases\n\n### 1. Extend third-party package migrations\n\nWhen you need to add custom migrations to third-party packages (e.g., to modify field constraints):\n\n```python\nMIGRATION_MODULES = {\n    # Redirect easy_thumbnails migrations to our controlled directory\n    # This allows us to extend/modify them without touching site-packages\n    \"easy_thumbnails\": \"myproject.apps.core.easy_thumbnails_migrations\",\n}\n```\n\n**Real-world example**: Increasing field length limits when the default constraints are too restrictive:\n\n```python\n# In your custom migration file\n# myproject/apps/core/easy_thumbnails_migrations/0003_increase_name_length.py\nfrom django.db import migrations, models\n\nclass Migration(migrations.Migration):\n    dependencies = [\n        ('easy_thumbnails', '0002_previous_migration'),\n    ]\n\n    operations = [\n        migrations.AlterField(\n            model_name='thumbnail',\n            name='name',\n            field=models.CharField(max_length=500),  # Increased from 255\n        ),\n    ]\n```\n\nThis approach is preferable when alternatives would be disruptive:\n- Changing thumbnail naming strategy would regenerate all existing thumbnails (wasteful at scale)\n- Modifying site-packages is fragile and lost on updates\n\n### 2. Disable migrations for an app\n\nSet the migration location to `None` to completely disable migrations for an app (useful in testing):\n\n```python\nMIGRATION_MODULES = {\n    'third_party_app': None,\n}\n```\n\n### 3. Centralize migrations\n\nKeep all migrations in a central location for better organization:\n\n```python\nMIGRATION_MODULES = {\n    'auth': 'myproject.migrations.auth',\n    'contenttypes': 'myproject.migrations.contenttypes',\n    'sessions': 'myproject.migrations.sessions',\n}\n```\n\n## Important considerations\n\n**Testing**: When `MIGRATION_MODULES` is set to `None` for an app, Django will create the tables from scratch using the current models without running migrations. This can speed up tests but may miss migration-related issues.\n\n**Deployment**: Ensure the custom migration directories exist and are included in your version control. Django will create migration files in the specified location when you run `makemigrations`.\n\n**Path structure**: The migration module path must be a valid Python import path. Create the necessary directory structure and `__init__.py` files.\n\n```bash\n# Example structure for custom migration location\nmyproject/\n\u251c\u2500\u2500 migrations/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 auth/\n\u2502   \u2502   \u2514\u2500\u2500 __init__.py\n\u2502   \u2514\u2500\u2500 third_party_app/\n\u2502       \u2514\u2500\u2500 __init__.py\n```\n\nUse `MIGRATION_MODULES` when you need fine-grained control over where Django stores and looks for migrations, especially when working with third-party packages or optimizing test performance.", "created": "2025-10-08T05:09:26+05:30", "created_utc": "2025-10-07T23:39:26+00:00", "updated": "2025-10-08T05:09:26+05:30", "updated_utc": "2025-10-07T23:39:26+00:00"}, {"path": "django_dynamic-related-name-abstract-models.md", "topic": "django", "title": "Dynamic related_name in Abstract Models", "url": "https://github.com/CuriousLearner/til/blob/main/django/dynamic-related-name-abstract-models.md", "body": "Django's abstract model inheritance supports template syntax for `related_name` that dynamically generates names based on the child class.\n\n## How it works\n\n-   `%(class)s` gets replaced with the lowercase name of the child model\n-   `%(app_label)s` gets replaced with the app name\n\n## Example\n\n```python\nclass BaseComment(models.Model):\n    author = models.ForeignKey(\n        settings.AUTH_USER_MODEL,\n        related_name=\"%(class)ss\",  # Template syntax\n        on_delete=models.CASCADE,\n    )\n    content = models.TextField()\n\n    class Meta:\n        abstract = True\n\n\nclass ArticleComment(BaseComment):  # related_name becomes \"articlecomments\"\n    article = models.ForeignKey(Article, on_delete=models.CASCADE)\n\n\nclass PhotoComment(BaseComment):  # related_name becomes \"photocomments\"\n    photo = models.ForeignKey(Photo, on_delete=models.CASCADE)\n```\n\nUsage:\n\n```python\nuser.articlecomments.all()  # All article comments by user\nuser.photocomments.all()    # All photo comments by user\n```\n\n## Why it's needed\n\nWithout dynamic related names, both child models would try to use the same `related_name`, causing a Django error:\n\n```\nERRORS: Reverse accessor clashes...\n```\n\n## Available placeholders\n\n| Placeholder     | Result                                        |\n| --------------- | --------------------------------------------- |\n| `%(class)s`     | Lowercase model name (e.g., `articlecomment`) |\n| `%(app_label)s` | App name (e.g., `blog`)                       |\n\nYou can combine them: `related_name=\"%(app_label)s_%(class)s_set\"`", "created": "2026-01-07T09:05:29+05:30", "created_utc": "2026-01-07T03:35:29+00:00", "updated": "2026-01-07T09:05:29+05:30", "updated_utc": "2026-01-07T03:35:29+00:00"}, {"path": "css_select-by-attr-in-list-of-values.md", "topic": "css", "title": "Select by attr in list of values", "url": "https://github.com/CuriousLearner/til/blob/main/css/select-by-attr-in-list-of-values.md", "body": "You can select an element in the DOM having an attribute with list of values.\n\nBy using the `[attr~='foo']` selector, it only select elements where the attribute contains the string \"foo\", and also takes into consideration that the element should contain an attr value in a space separated list.\n\nExample:\n\n```html\n<div attr='en-us foo bar' />\n```\n\ncan be selected with\n\n```css\n[attr~='foo'] { font-size:smaller; }\n```\n\nIf you want to select value with dash separated, you may do:\n\n```css\n/* value in a dash-separated list, e.g., \"-\" (U+002D) */\n[attr|='en'] { font-size:smaller; }\n```", "created": "2024-04-22T10:42:58+05:30", "created_utc": "2024-04-22T05:12:58+00:00", "updated": "2024-04-22T10:42:58+05:30", "updated_utc": "2024-04-22T05:12:58+00:00"}, {"path": "docker_get-rid-of-persistent-volume-docker-compose.md", "topic": "docker", "title": "Get rid of persistent volumes in `docker compose down`", "url": "https://github.com/CuriousLearner/til/blob/main/docker/get-rid-of-persistent-volume-docker-compose.md", "body": "```bash\ndocker compose down -v db\n```\n\n`docker-compose down`: This command stops and removes the containers, networks, volumes, and other resources created by `docker-compose up`. It is a convenient way to clean up resources after running Docker Compose services.\n\n`-v db`: The `-v` option instructs Docker Compose to remove the volume associated with the specified service (`db` in this case). This ensures that not only containers but also any persistent data stored in the volume is deleted. Useful when you want to clean up the entire database service, including its associated data volume, without affecting other services defined in the `docker-compose.yml`.", "created": "2024-06-23T09:47:51+05:30", "created_utc": "2024-06-23T04:17:51+00:00", "updated": "2025-02-28T07:22:58+05:30", "updated_utc": "2025-02-28T01:52:58+00:00"}, {"path": "docker_filter-list-of-containers.md", "topic": "docker", "title": "Filter list of docker containers", "url": "https://github.com/CuriousLearner/til/blob/main/docker/filter-list-of-containers.md", "body": "`docker ps` commands offer `-q` stands for `quiet`, that only displays container ids.\n\nThe `-f` flag is for filtering the list of running containers based on conditions provided.\n\nFor example:\n\n```bash\ndocker ps -qf \"name=django-web\"\n```\n\nwill list down the container id for container with name as `django-web`.\n\nYou may plug it into commands like:\n\n```bash\ndocker exec -it $(docker ps -qf \"name=django-web\") python manage.py migrate\n```\n\nto run the migrations from the django container.", "created": "2025-02-28T07:10:41+05:30", "created_utc": "2025-02-28T01:40:41+00:00", "updated": "2025-02-28T07:10:41+05:30", "updated_utc": "2025-02-28T01:40:41+00:00"}, {"path": "docker_target-in-docker-compose.md", "topic": "docker", "title": "Target in Docker Compose File", "url": "https://github.com/CuriousLearner/til/blob/main/docker/target-in-docker-compose.md", "body": "By associating specific stages with targets, you can build only the necessary components, reducing build times and optimizing resource usage.\n\nIf you have a Dockerfile, you can specify a target to build specific stages, like this:\n\n```docker\n# Stage 1: Build stage\nFROM python:3.7-slim as builder\n\nWORKDIR /app\n\nCOPY requirements.txt /app\n\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Stage 2: Runtime stage\nFROM python:3.7-slim as runtime\n\nWORKDIR /app\n\nCOPY --from=builder /usr/local/lib/python3.7/site-packages /usr/local/lib/python3.7/site-packages\nCOPY --from=builder /app /app\n\nCMD [\"python\", \"manage.py\", \"runserver\"]\n```\n\nThis Dockerfile separates the build process (`builder` stage) from the runtime environment (`runtime` stage), optimizing the final image size and dependencies.\n\nTo build and deploy only the runtime environment, define a target in your Docker Compose file:\n\n```yaml\nversion: '3'\n\nservices:\n  web:\n    build:\n      context: .\n      target: runtime\n```\n\nYou can then build your image with a specific target using Docker Compose:\n\n```bash\ndocker-compose build web\n```\n\nOr directly with docker build:\n\n```bash\ndocker build -t my_image . --target runtime\n```\n\nThis approach ensures you build only the necessary stages, improving efficiency.", "created": "2024-07-28T05:06:53+05:30", "created_utc": "2024-07-27T23:36:53+00:00", "updated": "2024-07-28T05:06:53+05:30", "updated_utc": "2024-07-27T23:36:53+00:00"}, {"path": "django-extensions_print-sql-on-orm-queries.md", "topic": "django-extensions", "title": "Print SQL on ORM queries", "url": "https://github.com/CuriousLearner/til/blob/main/django-extensions/print-sql-on-orm-queries.md", "body": "One way to know what SQL queries are being run on executing an ORM statement is with the following:\n\n```python\nfrom django.db import connection\n\nconnection.queries\n```\n\nDjango Extensions makes it easy to print all SQL queries if you pass an extra parameter while running shell like:\n\n```bash\npython manage.py shell_plus --print-sql\n```", "created": "2025-02-28T07:38:23+05:30", "created_utc": "2025-02-28T02:08:23+00:00", "updated": "2025-02-28T07:38:23+05:30", "updated_utc": "2025-02-28T02:08:23+00:00"}, {"path": "go_build-tags.md", "topic": "go", "title": "Build tags in Golang", "url": "https://github.com/CuriousLearner/til/blob/main/go/build-tags.md", "body": "A build tag is a line comment starting with // +build\n  and can be executed by `go build -tags=\"foo bar\"` command.\n  Build tags are placed before the package clause near or at the top of the file\n  followed by a blank line or other line comments.\n\nExample:\n\n```go\n// +build windows,!linux\npackage main\n...\n```\n\nIn this example, // +build windows,!linux is used to specify that this code should only be built on Windows and not on Linux.\n\nYou can also use multiple build tags by separating them with a comma.\n\nExample:\n\n```go\n// +build prod, dev, test\npackage main\n...\n```\n\nIn the above example, the code will be built only when running the command with either \"prod\" or \"dev\" or \"test\" as an argument.\n\n```bash\ngo build -tags=\"prod\"\n```", "created": "2024-06-23T09:04:03+05:30", "created_utc": "2024-06-23T03:34:03+00:00", "updated": "2024-06-23T09:04:03+05:30", "updated_utc": "2024-06-23T03:34:03+00:00"}, {"path": "go_import-unused-package.md", "topic": "go", "title": "Import package without using it via underscore (`_`)", "url": "https://github.com/CuriousLearner/til/blob/main/go/import-unused-package.md", "body": "In Go, using `_` in imports allows you to import packages solely for their side effects, bypassing the Go compiler's `unused import error`. This is particularly useful when you need to execute code in the imported package's `init()` functions or register with a global state.\n\nExample:\n```go\npackage main\n\nimport _ \"github.com/some/package\" // Importing for side effects\n\nfunc main() {\n    // Your main program logic\n}\n```", "created": "2024-06-23T09:40:10+05:30", "created_utc": "2024-06-23T04:10:10+00:00", "updated": "2024-06-23T09:40:10+05:30", "updated_utc": "2024-06-23T04:10:10+00:00"}, {"path": "go_go-doc-command.md", "topic": "go", "title": "Using `go doc` to Read Package Documentation", "url": "https://github.com/CuriousLearner/til/blob/main/go/go-doc-command.md", "body": "The `go doc` command lets you read Go package documentation directly from the terminal without opening a browser.\n\n## Basic Usage\n\n```bash\n# View documentation for an entire package\ngo doc fmt\n\n# View documentation for a specific function\ngo doc fmt.Println\n\n# View documentation for a type\ngo doc json.Encoder\n\n# View documentation for a method\ngo doc json.Encoder.Encode\n```\n\n## Useful Flags\n\n```bash\n# Show all exported symbols (not just the summary)\ngo doc -all fmt\n\n# Show source code alongside documentation\ngo doc -src fmt.Println\n\n# Show unexported symbols too\ngo doc -u fmt\n```\n\n## Examples\n\n```bash\n$ go doc fmt.Println\npackage fmt // import \"fmt\"\n\nfunc Println(a ...any) (n int, err error)\n    Println formats using the default formats for its operands and writes to\n    standard output. Spaces are always added between operands and a newline\n    is appended. It returns the number of bytes written and any write error\n    encountered.\n```\n\n## Why Use `go doc`?\n\n- **Fast** \u2013 No browser needed, stays in your terminal workflow\n- **Offline** \u2013 Works without internet once packages are downloaded\n- **Precise** \u2013 Jump directly to a specific function or type", "created": "2025-11-26T05:48:02+05:30", "created_utc": "2025-11-26T00:18:02+00:00", "updated": "2025-11-26T05:48:02+05:30", "updated_utc": "2025-11-26T00:18:02+00:00"}, {"path": "terragrunt_force-unlock-state.md", "topic": "terragrunt", "title": "Force unlock state file", "url": "https://github.com/CuriousLearner/til/blob/main/terragrunt/force-unlock-state.md", "body": "If you're dealing with a locked state in Terragrunt, you can unlock it using the following command:\n\n```bash\nterragrunt force-unlock <LOCK_ID>\n```\n\nThis action should be taken cautiously and only when you're certain that no ongoing `plan` or `apply` operations are running in Terragrunt.", "created": "2024-09-06T09:18:19+05:30", "created_utc": "2024-09-06T03:48:19+00:00", "updated": "2024-09-06T09:18:19+05:30", "updated_utc": "2024-09-06T03:48:19+00:00"}, {"path": "github-actions_permissions.md", "topic": "github-actions", "title": "Understanding Permissions in GitHub Actions", "url": "https://github.com/CuriousLearner/til/blob/main/github-actions/permissions.md", "body": "`permissions` in GitHub Actions control what a workflow can access and do within a repository. In your workflow file, you can define permissions like this:\n\n```yaml\npermissions:\n  contents: read\n  id-token: write\n  deployments: write\n```\n\n* `contents: read`: Allows the workflow to read repository contents.\n* `id-token: write`: Enables the workflow to request an OpenID Connect (OIDC) token for secure authentication with cloud providers.\n* `deployments: write`: Permits the workflow to create or update deployment statuses in GitHub.\n\nBy setting precise permissions, you adhere to the principle of least privilege, enhancing the security of your workflows.", "created": "2024-10-10T05:18:18+05:30", "created_utc": "2024-10-09T23:48:18+00:00", "updated": "2024-10-10T05:18:18+05:30", "updated_utc": "2024-10-09T23:48:18+00:00"}, {"path": "github-actions_path-filter.md", "topic": "github-actions", "title": "`paths` and `paths-ignore` filter for optimized Workflow run", "url": "https://github.com/CuriousLearner/til/blob/main/github-actions/path-filter.md", "body": "If you need your Github Actions to run only when changes happen to specific directories or files, you can add a `paths` filter in Github Actions like:\n\n```yaml\non:\n  push:\n    paths:\n      - 'src/**'\n      - 'scripts/**'\n```\n\nThis triggers the workflow if any changes are made in either the `src/` or `scripts/` directories.\n\nSimilarly, there is also `path-ignore` which can be used as:\n\n```yaml\non:\n  push:\n    paths-ignore:\n      - 'docs/**'\n      - '*.md'\n```\n\nThis will prevent the workflow from running when only changes to markdown files (`*.md`) or files inside the `docs/` folder are made.", "created": "2024-10-10T05:14:56+05:30", "created_utc": "2024-10-09T23:44:56+00:00", "updated": "2024-10-10T05:14:56+05:30", "updated_utc": "2024-10-09T23:44:56+00:00"}, {"path": "github-actions_ruff-github-comments.md", "topic": "github-actions", "title": "Enable automatic inline annotations in PR using Ruff", "url": "https://github.com/CuriousLearner/til/blob/main/github-actions/ruff-github-comments.md", "body": "To quickly see inline annotations from [ruff](https://github.com/charliermarsh/ruff) in your Github pull request, use `--format github` in Github Actions file like:\n\n```yaml\nname: CI\non: push\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v3\n      - name: Install Python\n        uses: actions/setup-python@v4\n        with:\n          python-version: \"3.11\"\n      - name: Install dependencies\n        run: |\n          python -m pip install --upgrade pip\n          pip install ruff\n      # Include `--format=github` to enable automatic inline annotations.\n      - name: Run Ruff\n        run: ruff check --format=github .\n```", "created": "2023-02-13T02:36:59+05:30", "created_utc": "2023-02-12T21:06:59+00:00", "updated": "2023-02-13T02:36:59+05:30", "updated_utc": "2023-02-12T21:06:59+00:00"}, {"path": "github-actions_push-to-repo.md", "topic": "github-actions", "title": "Permissions for pushing to a repository", "url": "https://github.com/CuriousLearner/til/blob/main/github-actions/push-to-repo.md", "body": "By default, the Github Action's bot has only read permission on the repository it is running the action on.\n\nIf you try to push to the repository through Github Actions' bot, [you'll get this error which I got on first build of my TIL repo](https://github.com/CuriousLearner/til/actions/runs/4153164170/jobs/7184624317): `remote: Permission to CuriousLearner/til.git denied to github-actions[bot].`\n\nUnder `Repository settings > Actions > General > Workflow permissions`, update permission scope to `Read and write permissions` which states `Workflows have read and write permissions in the repository for all scopes.`", "created": "2023-02-12T02:53:52+05:30", "created_utc": "2023-02-11T21:23:52+00:00", "updated": "2023-02-12T02:56:45+05:30", "updated_utc": "2023-02-11T21:26:45+00:00"}, {"path": "aws_no-self-signed-certificate-cloudfront.md", "topic": "aws", "title": "No self-signed certificates in Cloudfront's SSL connection", "url": "https://github.com/CuriousLearner/til/blob/main/aws/no-self-signed-certificate-cloudfront.md", "body": "If using cloudfront, then there will always be 2 SSL connections namely\n`Visitor => Cloudfront` and then `Cloudfront => origin`. Both of these will be publically signed certificate. You can never use self-signed certificate.", "created": "2023-02-12T03:13:44+05:30", "created_utc": "2023-02-11T21:43:44+00:00", "updated": "2023-02-12T03:13:44+05:30", "updated_utc": "2023-02-11T21:43:44+00:00"}, {"path": "aws_reserving-five-ips-per-subnet.md", "topic": "aws", "title": "AWS reserves five IPs per subnet", "url": "https://github.com/CuriousLearner/til/blob/main/aws/reserving-five-ips-per-subnet.md", "body": "Each subnet in AWS has 5 IP address registered. The following IP Address are registered as follows, (depending on the last octet or) ending with:\n\n- `.0` - Network Address\n- `.1` - VPC router\n- `.2` - DNS\n- `.3` - Reserved\n- `.255` - Broadcast Address", "created": "2023-02-12T03:25:56+05:30", "created_utc": "2023-02-11T21:55:56+00:00", "updated": "2023-02-12T03:25:56+05:30", "updated_utc": "2023-02-11T21:55:56+00:00"}, {"path": "aws_igw-vs-nat-gateway.md", "topic": "aws", "title": "IGW vs. NAT \u2013 Understanding Private to Public IP Mapping", "url": "https://github.com/CuriousLearner/til/blob/main/aws/igw-vs-nat-gateway.md", "body": "In AWS networking, Internet Gateway (IGW) and NAT Gateway (NAT) handle private-to-public IP mapping differently:\n\n- **Internet Gateway (IGW)**:\n\n    - Facilitates direct communication between instances in a public subnet and the internet.\n    - Each instance must have a public IP (assigned dynamically or via an Elastic IP (EIP)) to be directly accessible.\n    - Uses 1:1 private-to-public IP mapping, meaning each instance with a public IP has a unique public identity.\n\n- **NAT Gateway (NAT):**\n\n    - Used for instances in a private subnet to access the internet without being directly reachable from the outside.\n    - Implements IP masquerading (N:1), meaning multiple instances can share a single public IP to make outbound connections.\n    - Only supports outbound connections; inbound traffic is not directly possible.\n\n## Key Takeaway\n\n- Use IGW when instances need both inbound and outbound internet access.\n- Use NAT when private instances only need outbound internet access while remaining hidden from the public internet.", "created": "2025-02-28T10:23:00+05:30", "created_utc": "2025-02-28T04:53:00+00:00", "updated": "2025-12-08T09:54:37+05:30", "updated_utc": "2025-12-08T04:24:37+00:00"}, {"path": "aws_acm-cloudfront.md", "topic": "aws", "title": "ACM certificate should always be in us-east-1 for Cloudfront", "url": "https://github.com/CuriousLearner/til/blob/main/aws/acm-cloudfront.md", "body": "Since cloudfront is a global service, the certificate attached to it should always be in `us-east-1`.\n\nFor other services, that are regional, like Layer 7 - ALBs (Application Load Balancers), the region of SSL certificate depends on the region where ALB is placed.", "created": "2023-02-12T03:11:52+05:30", "created_utc": "2023-02-11T21:41:52+00:00", "updated": "2023-02-12T03:11:52+05:30", "updated_utc": "2023-02-11T21:41:52+00:00"}, {"path": "terraform_zip-lambda-code.md", "topic": "terraform", "title": "Zip Lambda code", "url": "https://github.com/CuriousLearner/til/blob/main/terraform/zip-lambda-code.md", "body": "You can configure terraform to zip-lambda function on triggers like:\n\n```bash\n# Zip the Lambda Function Code (local-exec example)\nresource \"null_resource\" \"zip_lambda\" {\n  provisioner \"local-exec\" {\n    command = \"zip lambda_function.zip lambda_function.py\"\n  }\n\n  triggers = {\n    always_run = \"${timestamp()}\"\n  }\n}\n```", "created": "2024-09-06T09:14:26+05:30", "created_utc": "2024-09-06T03:44:26+00:00", "updated": "2024-09-06T09:14:26+05:30", "updated_utc": "2024-09-06T03:44:26+00:00"}, {"path": "terraform_lifecycle-ignore-changes.md", "topic": "terraform", "title": "Lifecycle Ignore changes", "url": "https://github.com/CuriousLearner/til/blob/main/terraform/lifecycle-ignore-changes.md", "body": "> `lifecycle` is a nested block that can appear within a resource block. The `lifecycle` block and its contents are meta-arguments, available for all resource blocks regardless of type.\n\n> `ignore_changes` (list of attribute names) - By default, Terraform detects any difference in the current settings of a real infrastructure object and plans to update the remote object to match configuration.\n\n\nThis is helpful for example in `aws_ecs_service` resource when you want to avoid any changes in the task definition because the version number of the task definition will be out of terraform's cycle of managing the resource.\n\n```bash\nresource \"aws_ecs_service\" \"example\" {\n  name            = \"example-service\"\n  cluster         = aws_ecs_cluster.example.id\n  task_definition = aws_ecs_task_definition.example.arn\n  desired_count   = 1\n\n  lifecycle {\n    ignore_changes = [\n        task_definition, desired_count\n    ]\n  }\n}\n```\n\nThis also ignore the `desired_count` which is useful when you've autoscaling setup. So, tf doesn't want to be concerned on the number of tasks running.", "created": "2024-07-03T09:01:17+05:30", "created_utc": "2024-07-03T03:31:17+00:00", "updated": "2024-07-03T09:01:17+05:30", "updated_utc": "2024-07-03T03:31:17+00:00"}, {"path": "terraform_get-environment-variables.md", "topic": "terraform", "title": "Reading directly from environment variables", "url": "https://github.com/CuriousLearner/til/blob/main/terraform/get-environment-variables.md", "body": "We can use `get_env` function to directly read environment variables present. This means that the variables don't need to have `TF_VAR_` prefix. We are directly reading `TF_STATE_ROLE_ARN` if present in the env, and if not, then using the default value.\n\n\n```tf\nlocals {\n  # This will check the regular environment variable \"TF_CI_ROLE_ARN\"\n  tf_state_role_arn = get_env(\"TF_STATE_ROLE_ARN\", \"arn:aws:iam::XXXXXXXXX:role/terraform-state\")\n}\n\nremote_state {\n  backend = \"s3\"\n\n  config = {\n    bucket         = local.bucket\n    region         = local.region\n    key            = local.key\n    dynamodb_table = local.dynamodb_table\n    encrypt        = true\n    role_arn       = local.tf_state_role_arn\n  }\n}\n```", "created": "2025-02-28T07:10:41+05:30", "created_utc": "2025-02-28T01:40:41+00:00", "updated": "2025-02-28T07:10:41+05:30", "updated_utc": "2025-02-28T01:40:41+00:00"}, {"path": "bash_trap-err-exit-status.md", "topic": "bash", "title": "Bash trap with ERR and EXIT for Error Handling", "url": "https://github.com/CuriousLearner/til/blob/main/bash/trap-err-exit-status.md", "body": "The `trap` command in bash allows you to catch signals and execute code when certain events occur. It's particularly useful for cleanup operations and error notifications.\n\n## Basic Syntax\n\n```bash\ntrap 'command_to_execute' SIGNAL1 SIGNAL2 ...\n```\n\n## Understanding ERR and EXIT\n\n### ERR Signal\n\n-   Triggered when a command returns a non-zero exit status (fails)\n-   Only works when `set -e` is enabled or `set -E` for functions\n-   Does NOT trigger for commands in conditional statements (if, while, &&, ||)\n\n### EXIT Signal\n\n-   Always triggered when the script exits, regardless of success or failure\n-   Executes after ERR trap if both are set\n-   Perfect for cleanup operations\n\n## Example: Error Notification Pattern\n\n```bash\n#!/bin/bash\nSCRIPT_SUCCESS=false\n\nfunction notify_error {\n    if [ \"$SCRIPT_SUCCESS\" = false ]; then\n        echo \"Script failed, notifying Sentry...\"\n        curl -s \"${SENTRY_CRONS}?status=error\" || true\n    fi\n}\n\n# Trap both ERR and EXIT\ntrap notify_error ERR EXIT\n\n# Your script logic here\ndo_something_that_might_fail\n\n# If we reach here, mark as successful\nSCRIPT_SUCCESS=true\n```\n\n## How It Works\n\n1. **ERR trap**: If any command fails (returns non-zero), the trap function executes immediately\n2. **EXIT trap**: When the script exits (normally or due to error), the trap function executes\n3. The combination ensures notification happens whether the script:\n    - Fails at any point (ERR triggers, then EXIT)\n    - Completes successfully (only EXIT triggers, but `SCRIPT_SUCCESS=true` prevents notification)\n    - Is interrupted with Ctrl+C (EXIT still triggers)\n\n## Exit Status Codes\n\n-   `0`: Success\n-   `1-255`: Various error conditions\n-   `$?`: Contains the exit status of the last command\n\n## Best Practices\n\n1. Use `|| true` after non-critical commands in trap functions to prevent infinite loops\n2. Set flags like `SCRIPT_SUCCESS` to differentiate between normal and error exits\n3. Always use EXIT trap for cleanup (removing temp files, releasing locks)\n4. Use ERR trap for immediate error handling\n\n## Advanced Example\n\n```bash\n#!/bin/bash\nset -eE  # Exit on error, ERR trap inherits to functions\n\nTEMP_FILE=$(mktemp)\nLOCK_FILE=\"/var/run/myapp.lock\"\n\ncleanup() {\n    echo \"Cleaning up...\"\n    rm -f \"$TEMP_FILE\"\n    rm -f \"$LOCK_FILE\"\n}\n\nerror_handler() {\n    echo \"Error on line $LINENO: Command '$BASH_COMMAND' failed with exit code $?\"\n    # Additional error reporting\n}\n\ntrap cleanup EXIT\ntrap error_handler ERR\n\n# Create lock file\ntouch \"$LOCK_FILE\"\n\n# Main script logic\nprocess_data > \"$TEMP_FILE\"\nupload_results \"$TEMP_FILE\"\n\necho \"Script completed successfully\"\n```\n\n## Key Differences\n\n| Aspect          | ERR                  | EXIT                  |\n| --------------- | -------------------- | --------------------- |\n| When triggered  | On command failure   | Always on script exit |\n| Use case        | Error handling       | Cleanup operations    |\n| Execution order | First (if error)     | Always last           |\n| Works with      | `set -e` or `set -E` | Always works          |", "created": "2025-09-18T23:52:31+05:30", "created_utc": "2025-09-18T18:22:31+00:00", "updated": "2025-09-18T23:52:31+05:30", "updated_utc": "2025-09-18T18:22:31+00:00"}, {"path": "mastodon_send-direct-message.md", "topic": "mastodon", "title": "Sending direct message on Mastodon", "url": "https://github.com/CuriousLearner/til/blob/main/mastodon/send-direct-message.md", "body": "There are couple of ways to do this.\n\n1. Go on the profile of person who you want to send DM the DM to.  Click on ellipsis, and select `Direct message @<username>`. This would adjust the post visibility to automatically restrict to the person you're sending message. You can start posting as you want.\n\nIn the notifications, you'll see this post with `@` symbol indicating private message.\n\n2. The other way is to adjust your post settings on your own to `Mentioned people only`, and then start posting.`", "created": "2023-02-14T23:00:04+05:30", "created_utc": "2023-02-14T17:30:04+00:00", "updated": "2023-02-14T23:00:04+05:30", "updated_utc": "2023-02-14T17:30:04+00:00"}, {"path": "markdown_markdown-alerts.md", "topic": "markdown", "title": "Markdown Alerts", "url": "https://github.com/CuriousLearner/til/blob/main/markdown/markdown-alerts.md", "body": "Markdown alerts are a great way to emphasize important information in your documentation. Different types of alerts can be used to convey different levels of importance and types of information.\n\n## Markdown Alert Types\n\n### Note\n\n```bash\n> [!NOTE]\n> Highlights information that users should take into account, even when skimming.\n```\n\n### Tip\n\n```bash\n> [!TIP]\n> Optional information to help a user be more successful.\n```\n\n### Important\n\n```bash\n> [!IMPORTANT]\n> Crucial information necessary for users to succeed.\n```\n\n### Warning\n\n```bash\n> [!WARNING]\n> Critical content demanding immediate user attention due to potential risks.\n```\n\n### Caution\n\n```bash\n> [!CAUTION]\n> Negative potential consequences of an action.\n```\n\n## Example Usage\n\nHere\u2019s how you can use these alerts in a markdown document:\n\n### Note Example\n\n> [!NOTE]\n> Make sure to update your dependencies regularly to avoid security vulnerabilities.\n\n### Tip Example\n\n> [!TIP]\n> Use `envsubst` to easily substitute environment variables in your configuration files.\n\n### Important Example\n\n> [!IMPORTANT]\n> Always back up your data before performing major updates or changes.\n\n### Warning Example\n\n> [!WARNING]\n> Running this script will delete all files in the specified directory. Proceed with caution!\n\n### Caution Example\n\n> [!CAUTION]\n> Changing the configuration file might cause the application to become unstable if not done correctly.", "created": "2024-07-28T05:31:11+05:30", "created_utc": "2024-07-28T00:01:11+00:00", "updated": "2024-07-28T05:31:11+05:30", "updated_utc": "2024-07-28T00:01:11+00:00"}, {"path": "poetry_clear-cache-for-resolving-dependencies-faster.md", "topic": "poetry", "title": "Clear cache for resolving dependencies faster", "url": "https://github.com/CuriousLearner/til/blob/main/poetry/clear-cache-for-resolving-dependencies-faster.md", "body": "Well, I know caches are there for faster access. Unfortunately, poetry has a bug that is likely related to clearing out partial/incomplete/corrupted by concurrent usage downloads that can cause an indefinite hang while resolving dependencies.\n\nIf we clear the cache, it would be able to resolve dependencies faster.\n\nUse the following command for clearing the cache:\n\n```bash\npoetry cache clear --all pypi\n```\n\nMore [information on Github issue here](https://github.com/python-poetry/poetry/issues/2094#issuecomment-1248526469)", "created": "2023-02-16T18:18:33+05:30", "created_utc": "2023-02-16T12:48:33+00:00", "updated": "2025-02-28T07:22:58+05:30", "updated_utc": "2025-02-28T01:52:58+00:00"}, {"path": "poetry_editable-installation-for-package.md", "topic": "poetry", "title": "Editable installation for a package", "url": "https://github.com/CuriousLearner/til/blob/main/poetry/editable-installation-for-package.md", "body": "Just learned about editable installation in Python! It's a way to install a package in 'editable' mode, which means you can make changes to the package and see the changes reflected immediately without reinstalling it.\n\n```bash\npip install -e .\n```\n\nIn poetry, you can use the `path` option with `develop` as `true` to install a package in editable mode. This is useful when you want to develop a package and see the changes reflected immediately without reinstalling it.\n\n```bash\npacakge = {path= \"/path/to/package\", develop = true}\n```\n\nWith poetry, you can also use the `--editable` flag to install a package in editable mode.\n\n```bash\npoetry add --editable /path/to/package\n```", "created": "2025-02-28T07:10:41+05:30", "created_utc": "2025-02-28T01:40:41+00:00", "updated": "2025-12-08T09:54:37+05:30", "updated_utc": "2025-12-08T04:24:37+00:00"}, {"path": "poetry_generate-requirements-without-hashes.md", "topic": "poetry", "title": "Generate requirements without hashes", "url": "https://github.com/CuriousLearner/til/blob/main/poetry/generate-requirements-without-hashes.md", "body": "When using Docker, you can export dependencies to a `requirements.txt` file without hashes to decrease time to resolve dependencies.\n\n```bash\npoetry export --without-hashes --format=requirements.txt > requirements.txt\n```", "created": "2025-02-28T07:10:41+05:30", "created_utc": "2025-02-28T01:40:41+00:00", "updated": "2025-02-28T07:10:41+05:30", "updated_utc": "2025-02-28T01:40:41+00:00"}, {"path": "git_show-file-from-branch.md", "topic": "git", "title": "Show file contents from a specific branch", "url": "https://github.com/CuriousLearner/til/blob/main/git/show-file-from-branch.md", "body": "You can use `git show` to view a file's contents as they exist in a different branch without checking out that branch:\n\n```bash\ngit show <branch>:<path/to/file>\n```\n\n## Example\n\n```bash\ngit show feat/locations-geo-nearby:src/api/routers/locations.py\n```\n\nThis displays the full contents of `src/api/routers/locations.py` as it exists on the `feat/locations-geo-nearby` branch.\n\n## Redirecting stderr\n\nAdd `2>&1` to capture any error messages (like if the file doesn't exist on that branch) to stdout:\n\n```bash\ngit show feat/locations-geo-nearby:src/api/routers/locations.py 2>&1\n```\n\n## Use cases\n\n- Quickly inspect changes on another branch without switching branches\n- Compare implementations between branches\n- Extract specific file versions for review or debugging", "created": "2025-12-13T07:25:38+05:30", "created_utc": "2025-12-13T01:55:38+00:00", "updated": "2025-12-13T07:25:38+05:30", "updated_utc": "2025-12-13T01:55:38+00:00"}, {"path": "git_rev-parse-for-revision-information.md", "topic": "git", "title": "`git rev-parse` for parsing Git revision information", "url": "https://github.com/CuriousLearner/til/blob/main/git/rev-parse-for-revision-information.md", "body": "`git rev-parse` is a command used in Git, a distributed version control system. This command is used to parse Git revision information. It can be used to get the SHA-1 hash of a commit, the name of a branch, or the path of a file in the repository.\n\nHere are some examples of how to use `git rev-parse`:\n\n1. Get the SHA-1 hash of the current commit:\n\n   ```bash\n   git rev-parse HEAD\n   ```\n\n    This will output the SHA-1 hash of the current commit.\n\n2. Get the name of the current branch:\n\n    ```bash\n    git rev-parse --abbrev-ref HEAD\n    ```\n\n    This will output the name of the current branch.", "created": "2024-04-18T08:16:11+05:30", "created_utc": "2024-04-18T02:46:11+00:00", "updated": "2024-04-18T08:17:41+05:30", "updated_utc": "2024-04-18T02:47:41+00:00"}, {"path": "git_remove-files-from-git-history.md", "topic": "git", "title": "Remove Files from Git History", "url": "https://github.com/CuriousLearner/til/blob/main/git/remove-files-from-git-history.md", "body": "Use `git filter-branch --index-filter` to completely remove files from your entire git history. This is useful for removing accidentally committed secrets, large files, or sensitive data.\n\n## Remove a File from All History\n\n```bash\ngit filter-branch -f --index-filter \\\n  'git rm --cached --ignore-unmatch FILENAME' \\\n  --prune-empty -- --all\n```\n\n### Flags Explained\n\n| Flag               | Purpose                                                            |\n| ------------------ | ------------------------------------------------------------------ |\n| `-f`               | Force run, even if a backup from a previous filter-branch exists   |\n| `--index-filter`   | Operates on the index (staging area) \u2014 faster than `--tree-filter` |\n| `--cached`         | Remove from index only, not working directory                      |\n| `--ignore-unmatch` | Don't fail if file doesn't exist in some commits                   |\n| `--prune-empty`    | Remove commits that become empty after filtering                   |\n| `-- --all`         | Apply to all branches and tags                                     |\n\n## Suppress the Warning\n\nGit shows a warning that `filter-branch` has pitfalls. Suppress it with:\n\n```bash\nFILTER_BRANCH_SQUELCH_WARNING=1 git filter-branch ...\n```\n\n## Cleanup After Filtering\n\nAfter filter-branch, old objects still exist. Clean them up:\n\n```bash\n# Remove backup refs created by filter-branch\nrm -rf .git/refs/original/\n\n# Expire all reflog entries\ngit reflog expire --expire=now --all\n\n# Aggressively garbage collect\ngit gc --prune=now --aggressive\n```\n\n## Important Notes\n\n-   This rewrites history \u2014 force push required: `git push --force --all`\n-   Coordinate with team members; they'll need to re-clone or rebase\n-   Consider using `git-filter-repo` (faster, recommended by Git) for large repos", "created": "2025-12-08T06:16:48+05:30", "created_utc": "2025-12-08T00:46:48+00:00", "updated": "2025-12-08T06:16:48+05:30", "updated_utc": "2025-12-08T00:46:48+00:00"}, {"path": "git_diff-name-only.md", "topic": "git", "title": "`git diff --name-only` for file names only", "url": "https://github.com/CuriousLearner/til/blob/main/git/diff-name-only.md", "body": "`git diff --name-only` is a command used in Git, a distributed version control system. This command compares the working directory with the staging area (index) and outputs the names of files that have changed.\n\nThe `--name-only` flag is used to show only the names of the files that have changed, without any additional information. This can be useful when you want to see a list of files that have been modified, added, or deleted.\n\nHere is an example of how to use `git diff --name-only`:\n\n```bash\ngit diff --name-only\n```\n\nThis will output a list of file names that have changed between the working directory and the staging area.\n\nAlternate usage:\n\n```bash\ngit diff --name-only HEAD~1 HEAD\n```\n\nThis will output a list of file names that have changed between the current commit and the previous commit.\n\n```bash\ngit diff --name-only <commit1> <commit2>\n```\n\nor\n\n```bash\ngit diff --name-only <branch1> <branch2>\n```", "created": "2024-04-18T08:09:42+05:30", "created_utc": "2024-04-18T02:39:42+00:00", "updated": "2024-04-18T08:17:41+05:30", "updated_utc": "2024-04-18T02:47:41+00:00"}, {"path": "git_git-garbage-collection.md", "topic": "git", "title": "Git Garbage Collection", "url": "https://github.com/CuriousLearner/til/blob/main/git/git-garbage-collection.md", "body": "Use `git reflog expire` and `git gc` to permanently remove unreachable objects from your repository. Essential after rewriting history with filter-branch, reset, or rebase.\n\n## Expire Reflog Entries\n\n```bash\ngit reflog expire --expire=now --all\n```\n\n| Flag           | Purpose                                                |\n| -------------- | ------------------------------------------------------ |\n| `--expire=now` | Expire all entries immediately (default keeps 90 days) |\n| `--all`        | Process reflogs of all references, not just HEAD       |\n\nThe reflog keeps a safety net of all ref updates. Expiring it removes references to old commits, making them eligible for garbage collection.\n\n## Garbage Collect\n\n```bash\ngit gc --prune=now --aggressive\n```\n\n| Flag           | Purpose                                                       |\n| -------------- | ------------------------------------------------------------- |\n| `--prune=now`  | Remove unreachable objects immediately (default: 2 weeks old) |\n| `--aggressive` | More thorough optimization; slower but better compression     |\n\n## Full Cleanup Sequence\n\nAfter history-rewriting operations:\n\n```bash\n# 1. Remove filter-branch backups (if applicable)\nrm -rf .git/refs/original/\n\n# 2. Expire all reflog entries\ngit reflog expire --expire=now --all\n\n# 3. Aggressively garbage collect\ngit gc --prune=now --aggressive\n```\n\n## When to Use\n\n-   After `git filter-branch` to remove sensitive data\n-   After `git reset --hard` to older commits\n-   After deleting branches with large files\n-   To reduce repository size before sharing\n\n## Verify Objects Are Gone\n\n```bash\n# Check repository size before and after\ndu -sh .git\n\n# List all objects (should not include removed commits)\ngit rev-list --all\n```\n\n## Important Notes\n\n-   This is irreversible \u2014 unreachable objects cannot be recovered after gc\n-   `--aggressive` can be slow on large repos; omit for routine cleanup\n-   Remote repos retain their own objects until they run gc", "created": "2025-12-08T06:16:48+05:30", "created_utc": "2025-12-08T00:46:48+00:00", "updated": "2025-12-08T06:16:48+05:30", "updated_utc": "2025-12-08T00:46:48+00:00"}, {"path": "git_finding-commits-changing-a-file.md", "topic": "git", "title": "Finding Git Commits That Touched a Specific File or String", "url": "https://github.com/CuriousLearner/til/blob/main/git/finding-commits-changing-a-file.md", "body": "When working with **Git**, you may need to track changes to a file or find when a specific option (e.g., `-j` in a `Makefile`) was introduced or modified. Here are some useful commands:\n\n\n| Use Case                                          | Command                                               |\n| ------------------------------------------------- | ----------------------------------------------------- |\n| Show full commit history for a file               | `git log --follow -- Makefile`                        |\n| Show commit history with diffs                    | `git log --follow -p -- Makefile`                     |\n| Show only commits that added/removed `-j`         | `git log --follow -S'-j' -- Makefile`                 |\n| Show full details of commits affecting `-j`       | `git log --follow -S'-j' -p -- Makefile`              |\n| Find first commit that introduced `-j`            | `git log --follow --diff-filter=A -S'-j' -- Makefile` |\n| Show only the added/removed lines containing `-j` | `git log --follow -p -- Makefile \\| grep --color -E '^-.*-j \\|^\\+.*-j'` |\n\n**Key Flags Explained**:\n\n-   `--follow` - Continues tracking history even if the file was renamed.\n-   `-p` - Shows diffs (code changes).\n-   `-S'<string>'` - Finds commits where `<string>` was added or removed.\n-   `--diff-filter=A` - Finds the commit where the file or string first appeared.", "created": "2025-02-20T05:18:03+05:30", "created_utc": "2025-02-19T23:48:03+00:00", "updated": "2025-12-08T09:54:37+05:30", "updated_utc": "2025-12-08T04:24:37+00:00"}, {"path": "git_rewrite-git-commit-metadata.md", "topic": "git", "title": "Rewrite Git Commit Metadata", "url": "https://github.com/CuriousLearner/til/blob/main/git/rewrite-git-commit-metadata.md", "body": "Use `git filter-branch --env-filter` to rewrite commit metadata (author name, email, dates) across your entire git history. Useful for fixing incorrect author info or standardizing commit metadata.\n\n## Rewrite Using an External Script\n\n```bash\nFILTER_BRANCH_SQUELCH_WARNING=1 \\\ngit filter-branch -f --env-filter 'source /path/to/fix-script.sh' -- --all\n```\n\n### Example Script: Fix Author and Committer\n\n```bash\n# /tmp/fix-author-date.sh\n\nOLD_EMAIL=\"wrong@email.com\"\nNEW_NAME=\"Correct Name\"\nNEW_EMAIL=\"correct@email.com\"\n\nif [ \"$GIT_COMMITTER_EMAIL\" = \"$OLD_EMAIL\" ]; then\n    export GIT_COMMITTER_NAME=\"$NEW_NAME\"\n    export GIT_COMMITTER_EMAIL=\"$NEW_EMAIL\"\nfi\n\nif [ \"$GIT_AUTHOR_EMAIL\" = \"$OLD_EMAIL\" ]; then\n    export GIT_AUTHOR_NAME=\"$NEW_NAME\"\n    export GIT_AUTHOR_EMAIL=\"$NEW_EMAIL\"\nfi\n```\n\n## Available Environment Variables\n\nThe `--env-filter` can modify these variables:\n\n| Variable              | Purpose                             |\n| --------------------- | ----------------------------------- |\n| `GIT_AUTHOR_NAME`     | Author's name                       |\n| `GIT_AUTHOR_EMAIL`    | Author's email                      |\n| `GIT_AUTHOR_DATE`     | When the change was originally made |\n| `GIT_COMMITTER_NAME`  | Committer's name                    |\n| `GIT_COMMITTER_EMAIL` | Committer's email                   |\n| `GIT_COMMITTER_DATE`  | When the commit was created         |\n\n## Cleanup After Filtering\n\n```bash\nrm -rf .git/refs/original/\ngit reflog expire --expire=now --all\ngit gc --prune=now --aggressive\n```\n\n## Important Notes\n\n-   Author != Committer: Author wrote the code; Committer applied it (e.g., via cherry-pick)\n-   This rewrites history \u2014 force push required: `git push --force --all`\n-   All commit hashes will change", "created": "2025-12-08T06:16:48+05:30", "created_utc": "2025-12-08T00:46:48+00:00", "updated": "2025-12-08T06:16:48+05:30", "updated_utc": "2025-12-08T00:46:48+00:00"}, {"path": "git_revert-files-to-state-in-diff-branch.md", "topic": "git", "title": "Revert file(s) state to one in diff branch", "url": "https://github.com/CuriousLearner/til/blob/main/git/revert-files-to-state-in-diff-branch.md", "body": "Often times, we come across a situation where we have made changes in our current branch and want to revert some files to the exact state they were in at a certain point in another branch (diff branch). This can be done using `git`'s powerful features.\n\n```bash\ngit checkout <other-branch> -- <file-or-glob>...\n```\n\nTo check for changes before a checkout, you can use:\n\n```bash\ngit diff <commit-in-diff-branch> <file-or-glob>\n```\n\nExamples:\n\n```bash\n# Check 'README.md' diff from it's state in the master branch.\ngit diff master -- README.md\ngit checkout master -- README.md\n\n# Revert 'README.md' to the state it was in branch 'feature-branch'\ngit checkout feature-branch -- README.md\n```", "created": "2024-07-28T05:14:15+05:30", "created_utc": "2024-07-27T23:44:15+00:00", "updated": "2024-07-28T05:14:15+05:30", "updated_utc": "2024-07-27T23:44:15+00:00"}, {"path": "git_stash-selective-changes.md", "topic": "git", "title": "Stash Selective Changes in Git", "url": "https://github.com/CuriousLearner/til/blob/main/git/stash-selective-changes.md", "body": "Git allows stashing only staged or unstaged changes using `--staged` and `-k` (`--keep-index`). You can also stash specific files.\n\n## Stash Only Staged Changes\n\n```bash\ngit stash --staged\n```\n\n* Moves staged (indexed) changes to stash.\n* Leaves unstaged changes in the working directory.\n\n## Stash Only Unstaged Changes\n\n```bash\ngit stash -k  # or --keep-index\n```\n\n* Moves unstaged changes to stash.\n* Keeps staged changes for commit.\n\n## Stash Selected Files\n\n```bash\ngit stash push -m \"stash message\" -- path/to/file1 path/to/file2\n```\n\n* Stashes only the specified files.\n* Allows adding a message for reference.\n* Other changes remain untouched.", "created": "2025-02-20T08:07:06+05:30", "created_utc": "2025-02-20T02:37:06+00:00", "updated": "2025-12-08T09:54:37+05:30", "updated_utc": "2025-12-08T04:24:37+00:00"}, {"path": "git_merge-base-for-common-ancestor-commit.md", "topic": "git", "title": "`git merge-base` for common-ancestor commit", "url": "https://github.com/CuriousLearner/til/blob/main/git/merge-base-for-common-ancestor-commit.md", "body": "`git merge-base` is a command used in Git, a distributed version control system. This command finds the (most recent) common ancestor commit of two branches or commits.\n\nThe command is typically used in the format: `git merge-base <commit1> <commit2>` or `git merge-base <branch1> <branch2>`. This will output the SHA-1 hash of the common ancestor commit.\n\nFor example, if you have two branches, master and feature, and you want to find the commit where the feature branch diverged from master, you would use:\n\n```bash\ngit merge-base master feature\n```\n\nThis command is particularly useful when you want to see what has changed between two branches since they diverged. You can use the output of `git merge-base` as the argument to `git diff` to see these changes.\n\nRemember, `git merge-base` only shows the most recent common ancestor. If there are multiple common ancestors, it does not display them all.", "created": "2024-04-18T08:06:10+05:30", "created_utc": "2024-04-18T02:36:10+00:00", "updated": "2024-04-18T08:17:41+05:30", "updated_utc": "2024-04-18T02:47:41+00:00"}, {"path": "git_ignore-commits-in-git-blame.md", "topic": "git", "title": "Ignore commits in git-blame view", "url": "https://github.com/CuriousLearner/til/blob/main/git/ignore-commits-in-git-blame.md", "body": "> All revisions specified in the .git-blame-ignore-revs file, which must be in the root directory of your repository, are hidden from the blame view using Git's `git blame --ignore-revs-file` configuration setting.\n\nThe file can look like this, preferrably should always have comments:\n\n```bash\n# .git-blame-ignore-revs\n# Removed semi-colons from the entire codebase\na8940f7fbddf7fad9d7d50014d4e8d46baf30592\n# Converted all JavaScript to TypeScript\n69d029cec8337c616552756310748c4a507bd75a\n```\n\nOptionally, specify path for this ignore-revs-file, like:\n\n```bash\ngit blame --ignore-revs-file .git-blame-ignore-revs\n```\n\nYou can also configure your local git so it always ignores the revs in that file:\n\n```bash\ngit config blame.ignoreRevsFile .git-blame-ignore-revs\n```", "created": "2023-02-13T04:00:02+05:30", "created_utc": "2023-02-12T22:30:02+00:00", "updated": "2023-02-13T04:00:02+05:30", "updated_utc": "2023-02-12T22:30:02+00:00"}, {"path": "react_conditional-rendering.md", "topic": "react", "title": "Not to use && for conditional rendering", "url": "https://github.com/CuriousLearner/til/blob/main/react/conditional-rendering.md", "body": "Most tutorials on React will guide to use a pattern like this:\n\n```javascript\ncondition && <ConditionalComponent>\n```\n\nIf `condition` evaluates to `true`, `<ConditionalComponent>` will be rendered.\nIf `condition` evaluates to `false`, `<ConditionalComponent>` won't be rendered.\n\nWhile this works for majority of scenarios (for conditions that evaluate to `boolean`), it isn't a good practice.\n\nIf a condition evaluates to:\n- `0`, a `0` will be displayed in the UI prior to the rendered `ConditionalComponent`.\n- `undefined`, an exception will happen and UI will break: `\"Uncaught Error: Error(...): Nothing was returned from render. This usually means a return statement is missing. Or, to render nothing, return null.\"`\n\n\n## Best practice\n\nJavascript's ternary operator comes to rescue here. It would prevent the above issues from occuring\n\n```javascript\ncondition ? <ConditionalComponent /> : null\n```", "created": "2023-02-12T02:37:33+05:30", "created_utc": "2023-02-11T21:07:33+00:00", "updated": "2023-02-12T02:37:33+05:30", "updated_utc": "2023-02-11T21:07:33+00:00"}, {"path": "ssh_setup-ssh-tunneling.md", "topic": "ssh", "title": "Setup SSH Tunneling", "url": "https://github.com/CuriousLearner/til/blob/main/ssh/setup-ssh-tunneling.md", "body": "One plausible use case is accessing database in private subnet by creating SSH tunnel through the EC2 in public subnet like:\n\n```bash\nssh -L <local-port>:<database>.us-east-2.rds.amazonaws.com:<dbport> ec2-user@<host-ip>\n```\n\nNow you can access the DB on `local-port` in your system.", "created": "2023-07-08T01:17:46+05:30", "created_utc": "2023-07-07T19:47:46+00:00", "updated": "2025-02-28T07:22:58+05:30", "updated_utc": "2025-02-28T01:52:58+00:00"}, {"path": "vim_remove-lines-matching-pattern.md", "topic": "vim", "title": "Remove lines matching a pattern", "url": "https://github.com/CuriousLearner/til/blob/main/vim/remove-lines-matching-pattern.md", "body": "You can use `grep -v`, but let's do this in vim with Vim\u2019s powerful \"global\" command, `:g` for short as:\n\n- `:g/pattern/d` \u2013 Remove lines matching pattern\n- `:g!/pattern/d` \u2013 Remove lines that **do NOT** match the pattern\n- `:v/pattern/d` \u2013 Also removes lines that **do NOT** match the pattern", "created": "2023-02-13T02:48:35+05:30", "created_utc": "2023-02-12T21:18:35+00:00", "updated": "2023-02-13T02:48:35+05:30", "updated_utc": "2023-02-12T21:18:35+00:00"}, {"path": "vim_open-file-on-pattern.md", "topic": "vim", "title": "Open file on a particular pattern in file", "url": "https://github.com/CuriousLearner/til/blob/main/vim/open-file-on-pattern.md", "body": "Open a file directly to a pattern in Vim:\n\n`vim some_file.name +/your_pattern`\n\nlike:\n\n`vim .env +/DEBUG=`\n\nwill open `.env` file exactly on line where `DEBUG=` is mentioned.\n\nWhy? The pattern in your terminal history allows repetition. Useful for `.env` or `config` files.", "created": "2023-02-13T03:31:09+05:30", "created_utc": "2023-02-12T22:01:09+00:00", "updated": "2023-02-13T03:31:09+05:30", "updated_utc": "2023-02-12T22:01:09+00:00"}, {"path": "vim_text-objects-to-make-efficient-changes.md", "topic": "vim", "title": "Use text objects to make efficient changes", "url": "https://github.com/CuriousLearner/til/blob/main/vim/text-objects-to-make-efficient-changes.md", "body": "If you want to change something enclosed within brackets, curly braces, quotes, you can use:\n\n- `ci(` \u2013 **C**hange **I**nside brackets\n for changing\n\n```python\nmy_func(something)\n```\n\nto\n\n```python\nmy_func(something_else)\n```\n\n- `da\u201d` \u2013 **D**elete **A**round double quotes\n\n```python\nprint(\"Hello, World!\")\n```\n\nto\n\n```python\nprint()\n```\n\n- `di]` \u2013 **D**elete **I**nside square brackets\n\n```python\nmy_list = [1, 2, 3]\n```\n\nto\n\n```python\nmy_list = []\n```\n\n- `ci{` \u2013 **C**hange **I**nside curly braces\n\n```python\nmy_dict = {\"key\": \"value\"}\n```\n\nto\n\n```python\nmy_dict = {\"key\": \"new_value\"}\n```\n\n\n- `dap` \u2013 **D**elete **A**round **P**aragraph\n\n```python\ndef my_func():\n    print(\"Hello, World!\")\n```\n\nto\n\n```python\n```\n\n- `ciw` \u2013 **C**hange **I**nside **W**ord\n\n```python\nmy_var = \"value\"\n```\n\nto\n\n```python\nmy_var = \"new_value\"\n```\n\n\n\n- `vaw` \u2013 **V**isually select **A**round **W**ord\n\n```python\nmy_var = \"value\"\n```\n\nto\n\n```python\nmy_var = \"new_value\"\n```", "created": "2023-02-13T03:00:36+05:30", "created_utc": "2023-02-12T21:30:36+00:00", "updated": "2024-04-19T05:58:11+05:30", "updated_utc": "2024-04-19T00:28:11+00:00"}, {"path": "vim_indent-deindent-line.md", "topic": "vim", "title": "Indent and De-indent lines in Vim.", "url": "https://github.com/CuriousLearner/til/blob/main/vim/indent-deindent-line.md", "body": "This is very useful while dealing with Python code, or better yet, editing YAML config files for Kubernetes.\n\nPlace cursor at the start of line, press `Ctrl+v` to visually select and then press `j` to cover as many lines as you want to indent/de-indent.\n\nThen use `>` for indent and `<` for de-indent.\n\nIn insert mode, you can use `Ctrl+t` to indent and `Ctrl+d` to de-indent.", "created": "2023-02-13T03:40:39+05:30", "created_utc": "2023-02-12T22:10:39+00:00", "updated": "2023-02-13T03:40:39+05:30", "updated_utc": "2023-02-12T22:10:39+00:00"}, {"path": "vim_open-file-on-line.md", "topic": "vim", "title": "Open a file on a particular line", "url": "https://github.com/CuriousLearner/til/blob/main/vim/open-file-on-line.md", "body": "Use `vim <filename> +lineno` to open file on that line.\n\nFor example, `vim .env +33` will open `.env` file and place the cursor on line 33.", "created": "2023-02-13T03:54:14+05:30", "created_utc": "2023-02-12T22:24:14+00:00", "updated": "2023-02-13T03:54:14+05:30", "updated_utc": "2023-02-12T22:24:14+00:00"}, {"path": "yml_anchor-tags.md", "topic": "yml", "title": "YAML Anchor Tags using `<<:` and `*` Syntax", "url": "https://github.com/CuriousLearner/til/blob/main/yml/anchor-tags.md", "body": "YAML anchors help avoid repetition by allowing reuse of values using `&` (to define), `*` (to reference), and `<<:` (to merge).\n\n### Example with `*` Syntax\n\n```yaml\nbase: &default-settings\n  timeout: 30\n  retries: 5\n\nservice1:\n  settings: *default-settings\n  url: \"http://service1.com\"\n\nservice2:\n  settings: *default-settings\n  url: \"http://service2.com\"\n  retries: 3  # Override retries for service2\n```\n\n### Example with `<<:` for Merging\n\n```yaml\ndefaults: &defaults\n  timeout: 10\n  retries: 3\n\noverrides: &overrides\n  timeout: 20\n\ncombined:\n  <<: [*defaults, *overrides]\n  url: \"http://example.com\"\n```\n\n## Key Points\n\n- `*` References an anchor.\n- `<<:` Merges values from one or more anchors.\n- Anchors reduce duplication and simplify YAML configuration.", "created": "2024-10-10T05:06:56+05:30", "created_utc": "2024-10-09T23:36:56+00:00", "updated": "2024-10-10T05:06:56+05:30", "updated_utc": "2024-10-09T23:36:56+00:00"}, {"path": "make_parallel-jobs.md", "topic": "make", "title": "Speed Up Builds with `make -j` and Auto-Detect CPU Cores", "url": "https://github.com/CuriousLearner/til/blob/main/make/parallel-jobs.md", "body": "Parallelizing `make` builds can significantly reduce compile time. You can dynamically set the number of jobs based on available CPU cores.\n\n## Example Makefile\n\n```make\n# Detect number of available CPU cores\nNPROC := $(shell if command -v nproc >/dev/null; then nproc; else sysctl -n hw.ncpu; fi)\n# Use half the cores to avoid system overload\nNPROC_LIMIT := $(shell expr $(NPROC) / 2)\n\nall:\n\t@echo \"Building with $(NPROC_LIMIT) jobs...\"\n\tmake -j$(NPROC_LIMIT) target\n```\n\nThis `Makefile` dynamically sets the number of jobs to half the available CPU cores. You can adjust the `NPROC_LIMIT` variable to suit your system. Now, your builds will run faster by utilizing multiple cores.\n\nThe expression:\n\n```bash\n$(shell if command -v nproc >/dev/null; then nproc; else sysctl -n hw.ncpu; fi)\n```\n\nmakes sure the CPU cores are fetched using `nproc` command if available on Linux systems. If not, it falls back to using `sysctl -n hw.ncpu` on macOS.", "created": "2025-02-20T07:29:00+05:30", "created_utc": "2025-02-20T01:59:00+00:00", "updated": "2025-12-08T09:54:37+05:30", "updated_utc": "2025-12-08T04:24:37+00:00"}, {"path": "make_default-options-as-makeflags.md", "topic": "make", "title": "Control `make` Behavior with `MAKEFLAGS`", "url": "https://github.com/CuriousLearner/til/blob/main/make/default-options-as-makeflags.md", "body": "`MAKEFLAGS` is an environment variable that lets you set default options for `make`, avoiding the need to specify them every time.\n\n## Usage\n\nInstead of running:\n\n```bash\nmake -j$(nproc) --output-sync=target\n```\n\nYou can set `MAKEFLAGS` once:\n\n```bash\nexport MAKEFLAGS=\"-j$(nproc) --output-sync=target\"\nmake\n```\n\n## Common MAKEFLAGS Options\n* `-jN` \u2192 Run N parallel jobs (-j$(nproc) uses all CPU cores).\n* `--output-sync=target` \u2192 Keeps target outputs grouped.\n* `-s` \u2192 Silent mode (no command output).\n* `-k` \u2192 Keep going even if some targets fail.\n* `--warn-undefined-variables` \u2192 Debug undefined variables.\n\n## Example Makefile\n\n```make\nMAKEFLAGS += --warn-undefined-variables --environment-overrides --output-sync=target\n\nall:\n\t@echo \"Building with $(MAKEFLAGS)...\"\n\tmake build\n\nbuild:\n\t@echo \"Compiling...\"\n\tsleep 1\n\t@echo \"Done!\"\n```\n\nwould run as:\n\n```bash\nmake\n```\n\nand output\n\n```bash\nBuilding with  --warn-undefined-variables -e...\nmake build\nCompiling...\nsleep 1\nDone!\n```\n\n`MAKEFLAGS` helps streamline make commands for faster and cleaner builds!", "created": "2025-02-20T07:41:52+05:30", "created_utc": "2025-02-20T02:11:52+00:00", "updated": "2025-12-08T09:54:37+05:30", "updated_utc": "2025-12-08T04:24:37+00:00"}, {"path": "make_sync-output-for-parallel-jobs.md", "topic": "make", "title": "Improve `make` Output Readability with `--output-sync` for Parallel Jobs", "url": "https://github.com/CuriousLearner/til/blob/main/make/sync-output-for-parallel-jobs.md", "body": "When using parallel builds (`make -j`), the output from multiple jobs can get mixed, making logs difficult to follow. The `--output-sync` flag helps organize output for better readability.\n\n## Available `--output-sync` Options\n\n```bash\nmake -j$(nproc) --output-sync=[MODE]\n```\n\nModes:\n\n- `none` (default) \u2013 No synchronization, output may be interleaved.\n- `line` \u2013 Ensures each line of output is completed before switching jobs.\n- `target` \u2013 Groups output per target (useful for debugging).\n- `recurse` \u2013 Like target, but also applies to recursive make calls.\n\n## Example: Using `--output-sync=target`\n\n```bash\nmake -j$(nproc) --output-sync=target\n```\n\nThis keeps output of each target together & avoids mixing logs from multiple jobs. It makes errors easier to trace and improves readability in CI/CD pipelines.\n\n## Example Makefile\n\n```bash\nNPROC := $(shell if command -v nproc >/dev/null; then nproc; else sysctl -n hw.ncpu; fi)\n\nall:\n\t@echo \"Building with $(NPROC) jobs...\"\n\tmake -j$(NPROC) --output-sync=target build\n\nbuild:\n\t@echo \"Building target...\"\n\tsleep 1\n\t@echo \"Done!\"\n```", "created": "2025-02-20T07:34:34+05:30", "created_utc": "2025-02-20T02:04:34+00:00", "updated": "2025-12-08T09:54:37+05:30", "updated_utc": "2025-12-08T04:24:37+00:00"}]